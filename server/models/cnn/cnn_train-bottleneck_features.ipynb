{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/daniel/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/pandas/core/series.py:3727: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Workaround to make packages work in both Jupyter notebook and Python\n",
    "MODULE_ROOT_NAME = \"AgeEstimator\"\n",
    "MODULE_PATHS = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('../../..'))\n",
    "]\n",
    "MODULE_PATHS = list(\n",
    "    filter(lambda x: x.endswith(MODULE_ROOT_NAME), MODULE_PATHS))\n",
    "MODULE_PATH = MODULE_PATHS[0] if len(MODULE_PATHS) == 1 else \"\"\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from server.data.dataset import DataLoader\n",
    "from server.models.cnn.model import get_model, OLD_WEIGHTS_PATH, BEST_WEIGHTS_PATH, LABEL_MAPPING, get_models, N_CLASSES, IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as img\n",
    "import numpy as np\n",
    "import pandas\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_to_category_map():\n",
    "    unique_labels = list(set(LABEL_MAPPING.values()))\n",
    "    category_map = {class_label: inx for inx, class_label in enumerate(unique_labels)}\n",
    "    category_map_r = {inx: class_label for inx, class_label in enumerate(unique_labels)}\n",
    "    return category_map, category_map_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(y):\n",
    "    category_map, _ = get_label_to_category_map()\n",
    "    normalize = lambda x:category_map[LABEL_MAPPING[x]]\n",
    "    labels = np.vectorize(normalize)(y)\n",
    "    return to_categorical(labels, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_generators():\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    return train_datagen, valid_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_generator(datagen, dataframe, directory, batch_size=batch_size):\n",
    "    g = datagen.flow_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        directory=directory,\n",
    "        x_col=\"FilePath\",\n",
    "        y_col=\"Age\",\n",
    "        target_size=IMAGE_SIZE,\n",
    "        batch_size=batch_size,\n",
    "#         class_mode='sparse',\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "\n",
    "    # Convert to tf.data to better utilize multiprocessing\n",
    "    n_class = len(np.unique(np.array(dataframe[\"Age\"])))\n",
    "    tf_g = tf.data.Dataset.from_generator(lambda: g,\n",
    "        output_types=(tf.float32, tf.float32),\n",
    "        output_shapes=(\n",
    "            tf.TensorShape([None, IMAGE_SIZE[0], IMAGE_SIZE[1], 3]), \n",
    "            tf.TensorShape([None, 55])\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return tf_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and train/valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(x, y, name, sample_size=0):\n",
    "    # Stack to [[img, label], ...] matrix\n",
    "    stk = np.column_stack((x, y))\n",
    "    \n",
    "    # Save as csv\n",
    "    np.savetxt(\"%s.csv\" % (name), stk, fmt=\"%s\", delimiter=\",\", comments=\"\", header=\"FilePath,Age\")\n",
    "    \n",
    "    # `flow_from_dataframe` requires loading labels as string\n",
    "    df = pandas.read_csv(\"./%s.csv\" % (name), dtype=str)\n",
    "    \n",
    "    return df if sample_size == 0 else df.sample(n=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid(df):\n",
    "    train_df = df.sample(frac=0.9)\n",
    "    validation_df = df.drop(train_df.index)\n",
    "    return train_df, validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(log_dir):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "    # Don't waste our time/resource on bad training\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=100)\n",
    "    \n",
    "    tb = TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=False,\n",
    "        embeddings_freq=0,\n",
    "        embeddings_layer_names=None,\n",
    "        embeddings_metadata=None,\n",
    "        embeddings_data=None,\n",
    "        update_freq='epoch')\n",
    "    \n",
    "    # Save the best weight seen so far\n",
    "    mc = ModelCheckpoint(\n",
    "        BEST_WEIGHTS_PATH,\n",
    "#         monitor='val_loss',\n",
    "#         mode='min',\n",
    "        monitor='val_categorical_accuracy',\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True)\n",
    "    \n",
    "    # Modify the best score for retrains\n",
    "    mc.best = 0.14363\n",
    "    \n",
    "    # Try to get rid of local minimum\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=20,\n",
    "        min_lr=0.000001)\n",
    "    \n",
    "    return [mc, es, tb, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir():\n",
    "    log_i = 0\n",
    "    log_dir = \"logs/run_\"\n",
    "    \n",
    "    while os.path.exists(log_dir + str(log_i)):\n",
    "        log_i += 1\n",
    "\n",
    "    return log_dir + str(log_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(y_true, y_predict, top_n=5):\n",
    "    r\"\"\"Compare the last 10 result of top 5 prediction and its label.\"\"\"\n",
    "    y_hat = y_predict.argsort(axis=1)[:,-top_n:]\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    print(y_hat[-10:])\n",
    "    print(y_true[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a lot of models\n",
    "\n",
    "Train with a small portion of our dataset to compare the performace of the combinations of hyperparameters, so we can decide which model should be trained with a larger epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_many(train_generator, valid_generator, train_len, valid_len):\n",
    "    epochs = 20\n",
    "    models = get_models()\n",
    "    \n",
    "    for m in models:\n",
    "        model_name, optimizer, model = m\n",
    "        print(\"== Training %s ==\" % model_name)\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "                      metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "        log_dir = get_log_dir()\n",
    "        callbacks = get_callbacks(log_dir + \"/%s\" % model_name)\n",
    "\n",
    "        model.fit(\n",
    "            x=train_generator,\n",
    "            steps_per_epoch=train_len // batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_len // batch_size,\n",
    "            callbacks=callbacks,\n",
    "            workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "\n",
    "        model.save_weights(\"%s_weight.hdf5\" % model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the finalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y):\n",
    "    epochs = 1000\n",
    "    \n",
    "    optimizer = Nadam(lr=0.00007, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    model = get_model()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, \\\n",
    "        metrics=[\"categorical_accuracy\"])\n",
    "    \n",
    "    if os.path.exists(BEST_WEIGHTS_PATH):\n",
    "        model.load_weights(BEST_WEIGHTS_PATH)\n",
    "        print(\"best weight [%s] loaded.\" % BEST_WEIGHTS_PATH)\n",
    "#     elif os.path.exists(OLD_WEIGHTS_PATH):\n",
    "#         model.load_weights(OLD_WEIGHTS_PATH)\n",
    "#         print(\"old weight [%s] loaded.\" % OLD_WEIGHTS_PATH)\n",
    "    else:\n",
    "        print(\"fresh start.\")\n",
    "            \n",
    "    log_dir = get_log_dir()\n",
    "    callbacks = get_callbacks(log_dir)\n",
    "\n",
    "    train_len = round(len(x) * 0.9)\n",
    "    \n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        workers=max(2, multiprocessing.cpu_count() - 2),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "    \n",
    "    model.save_weights(OLD_WEIGHTS_PATH)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(sample_size=0, is_final_model=True):\n",
    "    dl = DataLoader()\n",
    "    use_bottleneck_features = True\n",
    "    x_train, y_train = dl.load_train(use_bottleneck_features)\n",
    "    x_test, y_test = dl.load_test(use_bottleneck_features)\n",
    "    \n",
    "    # Discretizate the continuous age into ordinal labels and map it with one-hot encoding\n",
    "    y_train = normalize_label(y_train)\n",
    "    y_test = normalize_label(y_test)\n",
    "    \n",
    "#     # The size is too large, so build a csv file for (image_filename/label) mapping\n",
    "#     train_df = get_dataframe(x_train, y_train, \"train\", sample_size=sample_size)\n",
    "#     train_df, valid_df = split_train_valid(train_df)\n",
    "#     test_df = get_dataframe(x_test, y_test, \"test\", sample_size=sample_size // 10)\n",
    "\n",
    "#     # Data augmentation for training set\n",
    "#     train_datagen, valid_datagen, test_datagen = get_img_generators()\n",
    "#     train_generator = to_generator(train_datagen, train_df, dl.train_dir)\n",
    "#     valid_generator = to_generator(valid_datagen, valid_df, dl.train_dir)\n",
    "#     test_generator = to_generator(test_datagen, test_df, dl.test_dir)\n",
    "    \n",
    "#     train_len = len(x_train)\n",
    "#     valid_len = len(valid_df)\n",
    "    test_len = len(x_test)\n",
    "    \n",
    "    if is_final_model:\n",
    "        # If it's a finalized model, train with a larger epochs\n",
    "        trained_model = train(x_train, y_train)\n",
    "\n",
    "        evaluation = trained_model.evaluate(\n",
    "            x=x_test, y=y_test)\n",
    "        y_hat = trained_model.predict(\n",
    "            x=x_test)\n",
    "        \n",
    "        print(evaluation)\n",
    "        compare_results(y_test, y_hat)\n",
    "\n",
    "        return evaluation, y_hat, y_test\n",
    "    \n",
    "#     else:\n",
    "#         train_many(train_generator, valid_generator, train_len, valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 2048)]            0         \n",
      "_________________________________________________________________\n",
      "d0 (Dense)                   (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dr1 (Dropout)                (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "d3 (Dense)                   (None, 55)                14135     \n",
      "=================================================================\n",
      "Total params: 2,775,607\n",
      "Trainable params: 2,772,023\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "best weight [best_nn_classification_weights.hdf5] loaded.\n",
      "Train on 134466 samples, validate on 14941 samples\n",
      "Epoch 1/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.3267 - categorical_accuracy: 0.5299\n",
      "Epoch 00001: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 15s 110us/sample - loss: 1.3268 - categorical_accuracy: 0.5297 - val_loss: 4.5870 - val_categorical_accuracy: 0.1427\n",
      "Epoch 2/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.3294 - categorical_accuracy: 0.5278\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.3295 - categorical_accuracy: 0.5278 - val_loss: 4.5875 - val_categorical_accuracy: 0.1423\n",
      "Epoch 3/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.3251 - categorical_accuracy: 0.5305\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3250 - categorical_accuracy: 0.5305 - val_loss: 4.5966 - val_categorical_accuracy: 0.1426\n",
      "Epoch 4/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.3251 - categorical_accuracy: 0.5302\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3250 - categorical_accuracy: 0.5303 - val_loss: 4.6014 - val_categorical_accuracy: 0.1414\n",
      "Epoch 5/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.3175 - categorical_accuracy: 0.5345\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.3174 - categorical_accuracy: 0.5345 - val_loss: 4.6117 - val_categorical_accuracy: 0.1414\n",
      "Epoch 6/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.3197 - categorical_accuracy: 0.5340\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3196 - categorical_accuracy: 0.5339 - val_loss: 4.6223 - val_categorical_accuracy: 0.1417\n",
      "Epoch 7/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.3109 - categorical_accuracy: 0.5325\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3111 - categorical_accuracy: 0.5324 - val_loss: 4.6286 - val_categorical_accuracy: 0.1416\n",
      "Epoch 8/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.3188 - categorical_accuracy: 0.5323\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3187 - categorical_accuracy: 0.5323 - val_loss: 4.6286 - val_categorical_accuracy: 0.1417\n",
      "Epoch 9/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.3149 - categorical_accuracy: 0.5320\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3150 - categorical_accuracy: 0.5320 - val_loss: 4.6414 - val_categorical_accuracy: 0.1415\n",
      "Epoch 10/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.3092 - categorical_accuracy: 0.5354\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3093 - categorical_accuracy: 0.5353 - val_loss: 4.6511 - val_categorical_accuracy: 0.1420\n",
      "Epoch 11/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.3126 - categorical_accuracy: 0.5351\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3128 - categorical_accuracy: 0.5351 - val_loss: 4.6471 - val_categorical_accuracy: 0.1412\n",
      "Epoch 12/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.3064 - categorical_accuracy: 0.5382\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3065 - categorical_accuracy: 0.5383 - val_loss: 4.6606 - val_categorical_accuracy: 0.1411\n",
      "Epoch 13/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.3050 - categorical_accuracy: 0.5368\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3052 - categorical_accuracy: 0.5368 - val_loss: 4.6680 - val_categorical_accuracy: 0.1412\n",
      "Epoch 14/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.3085 - categorical_accuracy: 0.5367\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3085 - categorical_accuracy: 0.5366 - val_loss: 4.6582 - val_categorical_accuracy: 0.1417\n",
      "Epoch 15/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.3074 - categorical_accuracy: 0.5381\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3075 - categorical_accuracy: 0.5382 - val_loss: 4.6617 - val_categorical_accuracy: 0.1424\n",
      "Epoch 16/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.3069 - categorical_accuracy: 0.5367\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3070 - categorical_accuracy: 0.5367 - val_loss: 4.6629 - val_categorical_accuracy: 0.1426\n",
      "Epoch 17/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.3014 - categorical_accuracy: 0.5377\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3015 - categorical_accuracy: 0.5377 - val_loss: 4.6655 - val_categorical_accuracy: 0.1419\n",
      "Epoch 18/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.3010 - categorical_accuracy: 0.5405\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.3014 - categorical_accuracy: 0.5404 - val_loss: 4.6728 - val_categorical_accuracy: 0.1414\n",
      "Epoch 19/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2949 - categorical_accuracy: 0.5396\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2954 - categorical_accuracy: 0.5395 - val_loss: 4.6825 - val_categorical_accuracy: 0.1416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2944 - categorical_accuracy: 0.5411\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2947 - categorical_accuracy: 0.5411 - val_loss: 4.6947 - val_categorical_accuracy: 0.1396\n",
      "Epoch 21/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2956 - categorical_accuracy: 0.5409\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2957 - categorical_accuracy: 0.5409 - val_loss: 4.6887 - val_categorical_accuracy: 0.1418\n",
      "Epoch 22/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2894 - categorical_accuracy: 0.5410\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2896 - categorical_accuracy: 0.5410 - val_loss: 4.7042 - val_categorical_accuracy: 0.1410\n",
      "Epoch 23/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2863 - categorical_accuracy: 0.5433\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2864 - categorical_accuracy: 0.5433 - val_loss: 4.7060 - val_categorical_accuracy: 0.1411\n",
      "Epoch 24/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.2892 - categorical_accuracy: 0.5431\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2892 - categorical_accuracy: 0.5432 - val_loss: 4.7053 - val_categorical_accuracy: 0.1404\n",
      "Epoch 25/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2829 - categorical_accuracy: 0.5456\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2832 - categorical_accuracy: 0.5456 - val_loss: 4.7144 - val_categorical_accuracy: 0.1409\n",
      "Epoch 26/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2868 - categorical_accuracy: 0.5429\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2870 - categorical_accuracy: 0.5428 - val_loss: 4.7133 - val_categorical_accuracy: 0.1418\n",
      "Epoch 27/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2827 - categorical_accuracy: 0.5454\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2829 - categorical_accuracy: 0.5453 - val_loss: 4.7016 - val_categorical_accuracy: 0.1415\n",
      "Epoch 28/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 1.2784 - categorical_accuracy: 0.5467\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2785 - categorical_accuracy: 0.5467 - val_loss: 4.7154 - val_categorical_accuracy: 0.1420\n",
      "Epoch 29/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2863 - categorical_accuracy: 0.5447\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2861 - categorical_accuracy: 0.5447 - val_loss: 4.7163 - val_categorical_accuracy: 0.1407\n",
      "Epoch 30/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2828 - categorical_accuracy: 0.5439\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2828 - categorical_accuracy: 0.5439 - val_loss: 4.7156 - val_categorical_accuracy: 0.1423\n",
      "Epoch 31/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2830 - categorical_accuracy: 0.5453\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2831 - categorical_accuracy: 0.5453 - val_loss: 4.7215 - val_categorical_accuracy: 0.1412\n",
      "Epoch 32/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2820 - categorical_accuracy: 0.5454\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 100us/sample - loss: 1.2820 - categorical_accuracy: 0.5454 - val_loss: 4.7127 - val_categorical_accuracy: 0.1410\n",
      "Epoch 33/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2826 - categorical_accuracy: 0.5445\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2827 - categorical_accuracy: 0.5445 - val_loss: 4.7234 - val_categorical_accuracy: 0.1408\n",
      "Epoch 34/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2821 - categorical_accuracy: 0.5457\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2821 - categorical_accuracy: 0.5457 - val_loss: 4.7384 - val_categorical_accuracy: 0.1412\n",
      "Epoch 35/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2787 - categorical_accuracy: 0.5461\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2786 - categorical_accuracy: 0.5462 - val_loss: 4.7271 - val_categorical_accuracy: 0.1420\n",
      "Epoch 36/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.2849 - categorical_accuracy: 0.5428\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2851 - categorical_accuracy: 0.5427 - val_loss: 4.7381 - val_categorical_accuracy: 0.1416\n",
      "Epoch 37/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 1.2839 - categorical_accuracy: 0.5453\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2839 - categorical_accuracy: 0.5453 - val_loss: 4.7312 - val_categorical_accuracy: 0.1397\n",
      "Epoch 38/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2806 - categorical_accuracy: 0.5464\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2807 - categorical_accuracy: 0.5464 - val_loss: 4.7254 - val_categorical_accuracy: 0.1411\n",
      "Epoch 39/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2788 - categorical_accuracy: 0.5455\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2788 - categorical_accuracy: 0.5455 - val_loss: 4.7320 - val_categorical_accuracy: 0.1412\n",
      "Epoch 40/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2805 - categorical_accuracy: 0.5453\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2804 - categorical_accuracy: 0.5451 - val_loss: 4.7292 - val_categorical_accuracy: 0.1406\n",
      "Epoch 41/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 1.2765 - categorical_accuracy: 0.5471\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2766 - categorical_accuracy: 0.5470 - val_loss: 4.7359 - val_categorical_accuracy: 0.1412\n",
      "Epoch 42/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2777 - categorical_accuracy: 0.5486\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2777 - categorical_accuracy: 0.5486 - val_loss: 4.7329 - val_categorical_accuracy: 0.1407\n",
      "Epoch 43/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2729 - categorical_accuracy: 0.5475\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2725 - categorical_accuracy: 0.5477 - val_loss: 4.7341 - val_categorical_accuracy: 0.1404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2760 - categorical_accuracy: 0.5464\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2759 - categorical_accuracy: 0.5464 - val_loss: 4.7375 - val_categorical_accuracy: 0.1409\n",
      "Epoch 45/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2743 - categorical_accuracy: 0.5488\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2747 - categorical_accuracy: 0.5487 - val_loss: 4.7392 - val_categorical_accuracy: 0.1405\n",
      "Epoch 46/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.2750 - categorical_accuracy: 0.5492\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2755 - categorical_accuracy: 0.5490 - val_loss: 4.7549 - val_categorical_accuracy: 0.1408\n",
      "Epoch 47/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2754 - categorical_accuracy: 0.5457\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2756 - categorical_accuracy: 0.5456 - val_loss: 4.7392 - val_categorical_accuracy: 0.1406\n",
      "Epoch 48/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2739 - categorical_accuracy: 0.5454\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2740 - categorical_accuracy: 0.5454 - val_loss: 4.7452 - val_categorical_accuracy: 0.1418\n",
      "Epoch 49/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2789 - categorical_accuracy: 0.5465\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2790 - categorical_accuracy: 0.5465 - val_loss: 4.7347 - val_categorical_accuracy: 0.1416\n",
      "Epoch 50/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2776 - categorical_accuracy: 0.5487\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2777 - categorical_accuracy: 0.5487 - val_loss: 4.7439 - val_categorical_accuracy: 0.1414\n",
      "Epoch 51/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2744 - categorical_accuracy: 0.5483\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2743 - categorical_accuracy: 0.5483 - val_loss: 4.7471 - val_categorical_accuracy: 0.1409\n",
      "Epoch 52/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2765 - categorical_accuracy: 0.5475\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2766 - categorical_accuracy: 0.5475 - val_loss: 4.7367 - val_categorical_accuracy: 0.1409\n",
      "Epoch 53/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2762 - categorical_accuracy: 0.5456\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2763 - categorical_accuracy: 0.5456 - val_loss: 4.7531 - val_categorical_accuracy: 0.1406\n",
      "Epoch 54/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2710 - categorical_accuracy: 0.5484\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2712 - categorical_accuracy: 0.5484 - val_loss: 4.7555 - val_categorical_accuracy: 0.1421\n",
      "Epoch 55/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2733 - categorical_accuracy: 0.5486\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2735 - categorical_accuracy: 0.5485 - val_loss: 4.7533 - val_categorical_accuracy: 0.1416\n",
      "Epoch 56/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2733 - categorical_accuracy: 0.5471\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2735 - categorical_accuracy: 0.5470 - val_loss: 4.7437 - val_categorical_accuracy: 0.1408\n",
      "Epoch 57/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2733 - categorical_accuracy: 0.5475\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 102us/sample - loss: 1.2735 - categorical_accuracy: 0.5474 - val_loss: 4.7414 - val_categorical_accuracy: 0.1412\n",
      "Epoch 58/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2748 - categorical_accuracy: 0.5479\n",
      "Epoch 00058: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2750 - categorical_accuracy: 0.5479 - val_loss: 4.7485 - val_categorical_accuracy: 0.1408\n",
      "Epoch 59/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2741 - categorical_accuracy: 0.5495\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 102us/sample - loss: 1.2742 - categorical_accuracy: 0.5494 - val_loss: 4.7427 - val_categorical_accuracy: 0.1413\n",
      "Epoch 60/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2775 - categorical_accuracy: 0.5474\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 104us/sample - loss: 1.2776 - categorical_accuracy: 0.5474 - val_loss: 4.7534 - val_categorical_accuracy: 0.1406\n",
      "Epoch 61/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2731 - categorical_accuracy: 0.5484\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 102us/sample - loss: 1.2732 - categorical_accuracy: 0.5483 - val_loss: 4.7402 - val_categorical_accuracy: 0.1417\n",
      "Epoch 62/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.2727 - categorical_accuracy: 0.5478\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2728 - categorical_accuracy: 0.5477 - val_loss: 4.7474 - val_categorical_accuracy: 0.1397\n",
      "Epoch 63/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2757 - categorical_accuracy: 0.5450\n",
      "Epoch 00063: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 103us/sample - loss: 1.2757 - categorical_accuracy: 0.5450 - val_loss: 4.7400 - val_categorical_accuracy: 0.1408\n",
      "Epoch 64/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2737 - categorical_accuracy: 0.5485\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 107us/sample - loss: 1.2736 - categorical_accuracy: 0.5486 - val_loss: 4.7605 - val_categorical_accuracy: 0.1423\n",
      "Epoch 65/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2726 - categorical_accuracy: 0.5471\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 15s 108us/sample - loss: 1.2726 - categorical_accuracy: 0.5471 - val_loss: 4.7390 - val_categorical_accuracy: 0.1414\n",
      "Epoch 66/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2727 - categorical_accuracy: 0.5488\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 103us/sample - loss: 1.2727 - categorical_accuracy: 0.5488 - val_loss: 4.7509 - val_categorical_accuracy: 0.1412\n",
      "Epoch 67/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2704 - categorical_accuracy: 0.5491\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 103us/sample - loss: 1.2705 - categorical_accuracy: 0.5491 - val_loss: 4.7564 - val_categorical_accuracy: 0.1416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2709 - categorical_accuracy: 0.5492\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 106us/sample - loss: 1.2710 - categorical_accuracy: 0.5491 - val_loss: 4.7465 - val_categorical_accuracy: 0.1417\n",
      "Epoch 69/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2726 - categorical_accuracy: 0.5481\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 102us/sample - loss: 1.2726 - categorical_accuracy: 0.5481 - val_loss: 4.7472 - val_categorical_accuracy: 0.1422\n",
      "Epoch 70/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2732 - categorical_accuracy: 0.5465\n",
      "Epoch 00070: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2732 - categorical_accuracy: 0.5464 - val_loss: 4.7514 - val_categorical_accuracy: 0.1412\n",
      "Epoch 71/1000\n",
      "134464/134466 [============================>.] - ETA: 0s - loss: 1.2747 - categorical_accuracy: 0.5464\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2748 - categorical_accuracy: 0.5464 - val_loss: 4.7491 - val_categorical_accuracy: 0.1408\n",
      "Epoch 72/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2684 - categorical_accuracy: 0.5483\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2684 - categorical_accuracy: 0.5483 - val_loss: 4.7365 - val_categorical_accuracy: 0.1417\n",
      "Epoch 73/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2776 - categorical_accuracy: 0.5456\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 108us/sample - loss: 1.2779 - categorical_accuracy: 0.5455 - val_loss: 4.7627 - val_categorical_accuracy: 0.1414\n",
      "Epoch 74/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2707 - categorical_accuracy: 0.5488\n",
      "Epoch 00074: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2708 - categorical_accuracy: 0.5488 - val_loss: 4.7471 - val_categorical_accuracy: 0.1414\n",
      "Epoch 75/1000\n",
      "133952/134466 [============================>.] - ETA: 0s - loss: 1.2732 - categorical_accuracy: 0.5481\n",
      "Epoch 00075: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 98us/sample - loss: 1.2732 - categorical_accuracy: 0.5481 - val_loss: 4.7418 - val_categorical_accuracy: 0.1416\n",
      "Epoch 76/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2722 - categorical_accuracy: 0.5464\n",
      "Epoch 00076: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2723 - categorical_accuracy: 0.5463 - val_loss: 4.7677 - val_categorical_accuracy: 0.1409\n",
      "Epoch 77/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2715 - categorical_accuracy: 0.5497\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 98us/sample - loss: 1.2714 - categorical_accuracy: 0.5497 - val_loss: 4.7550 - val_categorical_accuracy: 0.1422\n",
      "Epoch 78/1000\n",
      "133952/134466 [============================>.] - ETA: 0s - loss: 1.2752 - categorical_accuracy: 0.5475\n",
      "Epoch 00078: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2755 - categorical_accuracy: 0.5473 - val_loss: 4.7524 - val_categorical_accuracy: 0.1406\n",
      "Epoch 79/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2741 - categorical_accuracy: 0.5485\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2742 - categorical_accuracy: 0.5484 - val_loss: 4.7488 - val_categorical_accuracy: 0.1399\n",
      "Epoch 80/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2768 - categorical_accuracy: 0.5471\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2768 - categorical_accuracy: 0.5471 - val_loss: 4.7570 - val_categorical_accuracy: 0.1409\n",
      "Epoch 81/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2729 - categorical_accuracy: 0.5474\n",
      "Epoch 00081: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 102us/sample - loss: 1.2728 - categorical_accuracy: 0.5474 - val_loss: 4.7458 - val_categorical_accuracy: 0.1418\n",
      "Epoch 82/1000\n",
      "133952/134466 [============================>.] - ETA: 0s - loss: 1.2705 - categorical_accuracy: 0.5484\n",
      "Epoch 00082: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 103us/sample - loss: 1.2702 - categorical_accuracy: 0.5485 - val_loss: 4.7547 - val_categorical_accuracy: 0.1410\n",
      "Epoch 83/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2721 - categorical_accuracy: 0.5483\n",
      "Epoch 00083: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2720 - categorical_accuracy: 0.5484 - val_loss: 4.7439 - val_categorical_accuracy: 0.1412\n",
      "Epoch 84/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2745 - categorical_accuracy: 0.5460\n",
      "Epoch 00084: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2746 - categorical_accuracy: 0.5460 - val_loss: 4.7495 - val_categorical_accuracy: 0.1406\n",
      "Epoch 85/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2730 - categorical_accuracy: 0.5486\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2731 - categorical_accuracy: 0.5485 - val_loss: 4.7501 - val_categorical_accuracy: 0.1410\n",
      "Epoch 86/1000\n",
      "134080/134466 [============================>.] - ETA: 0s - loss: 1.2721 - categorical_accuracy: 0.5475\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2724 - categorical_accuracy: 0.5474 - val_loss: 4.7468 - val_categorical_accuracy: 0.1418\n",
      "Epoch 87/1000\n",
      "133952/134466 [============================>.] - ETA: 0s - loss: 1.2737 - categorical_accuracy: 0.5497\n",
      "Epoch 00087: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2740 - categorical_accuracy: 0.5496 - val_loss: 4.7472 - val_categorical_accuracy: 0.1410\n",
      "Epoch 88/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2726 - categorical_accuracy: 0.5489\n",
      "Epoch 00088: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2727 - categorical_accuracy: 0.5489 - val_loss: 4.7639 - val_categorical_accuracy: 0.1407\n",
      "Epoch 89/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2715 - categorical_accuracy: 0.5491\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2717 - categorical_accuracy: 0.5490 - val_loss: 4.7643 - val_categorical_accuracy: 0.1420\n",
      "Epoch 90/1000\n",
      "134016/134466 [============================>.] - ETA: 0s - loss: 1.2735 - categorical_accuracy: 0.5473\n",
      "Epoch 00090: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 98us/sample - loss: 1.2734 - categorical_accuracy: 0.5474 - val_loss: 4.7517 - val_categorical_accuracy: 0.1406\n",
      "Epoch 91/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2736 - categorical_accuracy: 0.5474\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2735 - categorical_accuracy: 0.5474 - val_loss: 4.7561 - val_categorical_accuracy: 0.1408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2728 - categorical_accuracy: 0.5466\n",
      "Epoch 00092: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2728 - categorical_accuracy: 0.5466 - val_loss: 4.7525 - val_categorical_accuracy: 0.1416\n",
      "Epoch 93/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.2728 - categorical_accuracy: 0.5480\n",
      "Epoch 00093: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 97us/sample - loss: 1.2728 - categorical_accuracy: 0.5481 - val_loss: 4.7485 - val_categorical_accuracy: 0.1410\n",
      "Epoch 94/1000\n",
      "134336/134466 [============================>.] - ETA: 0s - loss: 1.2753 - categorical_accuracy: 0.5484\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 99us/sample - loss: 1.2753 - categorical_accuracy: 0.5485 - val_loss: 4.7511 - val_categorical_accuracy: 0.1413\n",
      "Epoch 95/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2701 - categorical_accuracy: 0.5486\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2702 - categorical_accuracy: 0.5486 - val_loss: 4.7534 - val_categorical_accuracy: 0.1412\n",
      "Epoch 96/1000\n",
      "134208/134466 [============================>.] - ETA: 0s - loss: 1.2734 - categorical_accuracy: 0.5463\n",
      "Epoch 00096: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 102us/sample - loss: 1.2736 - categorical_accuracy: 0.5462 - val_loss: 4.7537 - val_categorical_accuracy: 0.1400\n",
      "Epoch 97/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2744 - categorical_accuracy: 0.5480\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 101us/sample - loss: 1.2746 - categorical_accuracy: 0.5480 - val_loss: 4.7463 - val_categorical_accuracy: 0.1405\n",
      "Epoch 98/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2722 - categorical_accuracy: 0.5484\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 104us/sample - loss: 1.2723 - categorical_accuracy: 0.5482 - val_loss: 4.7494 - val_categorical_accuracy: 0.1412\n",
      "Epoch 99/1000\n",
      "134144/134466 [============================>.] - ETA: 0s - loss: 1.2747 - categorical_accuracy: 0.5468\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 104us/sample - loss: 1.2747 - categorical_accuracy: 0.5468 - val_loss: 4.7587 - val_categorical_accuracy: 0.1402\n",
      "Epoch 100/1000\n",
      "134400/134466 [============================>.] - ETA: 0s - loss: 1.2702 - categorical_accuracy: 0.5493\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 14s 106us/sample - loss: 1.2704 - categorical_accuracy: 0.5492 - val_loss: 4.7517 - val_categorical_accuracy: 0.1409\n",
      "Epoch 101/1000\n",
      "134272/134466 [============================>.] - ETA: 0s - loss: 1.2697 - categorical_accuracy: 0.5494\n",
      "Epoch 00101: val_categorical_accuracy did not improve from 0.14363\n",
      "134466/134466 [==============================] - 13s 100us/sample - loss: 1.2698 - categorical_accuracy: 0.5493 - val_loss: 4.7634 - val_categorical_accuracy: 0.1414\n",
      "Epoch 00101: early stopping\n",
      "37349/37349 [==============================] - 1s 35us/sample - loss: 4.7560 - categorical_accuracy: 0.1367\n",
      "[4.756029250111177, 0.13668372]\n",
      "[[31 40 34 24 39]\n",
      " [13  4  7 12  6]\n",
      " [23 19 20 22 21]\n",
      " [41 45 44 42 43]\n",
      " [28 25 24 26 27]\n",
      " [18 19 20 22 21]\n",
      " [37 36 33 35 34]\n",
      " [47 41 48 46 33]\n",
      " [11  9 10 12 13]\n",
      " [ 6 20  7  4  5]]\n",
      "[28 13 24 50 25 19 30 30  7 22]\n"
     ]
    }
   ],
   "source": [
    "res = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
