{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Workaround to make packages work in both Jupyter notebook and Python\n",
    "module_root_name = \"AgeEstimator\"\n",
    "module_paths = [\n",
    "    os.path.abspath(os.path.join('..')),\n",
    "    os.path.abspath(os.path.join('../..')),\n",
    "    os.path.abspath(os.path.join('../../..'))\n",
    "]\n",
    "module_paths = list(filter(lambda x: x.endswith(module_root_name), module_paths))\n",
    "module_path = module_paths[0] if len(module_paths) == 1 else \"\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# from server.models.cnn.data_loader import DataLoader\n",
    "\n",
    "from server.data.dataset import DataLoader\n",
    "\n",
    "dl = DataLoader()\n",
    "x_train, y_train = dl.load_train()\n",
    "x_test, y_test = dl.load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import filters\n",
    "from pylab import *\n",
    "from PIL import Image, ImageFilter \n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  age\n",
       "0  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   17\n",
       "1  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   39\n",
       "2  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   26\n",
       "3  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   29\n",
       "4  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create csv files\n",
    "df_train = pd.DataFrame(x_train, columns=['filename'])\n",
    "df_train['age'] = pd.Series(y_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/xiaoxiao/Desktop/SCU/2020winter/coen281...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  age\n",
       "0  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   26\n",
       "1  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   29\n",
       "2  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   38\n",
       "3  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   30\n",
       "4  /Users/xiaoxiao/Desktop/SCU/2020winter/coen281...   23"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(x_test, columns=['filename'])\n",
    "df_test['age'] = pd.Series(y_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('training_set.csv', index=False)\n",
    "df_test.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 149724\n",
      "Number of test examples: 37430\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {df_train.shape[0]}')\n",
    "print(f'Number of test examples: {df_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 110\n"
     ]
    }
   ],
   "source": [
    "train_max = df_train.max()['age']\n",
    "test_max = df_test.max()['age']\n",
    "print(train_max, test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of age labels: 102\n"
     ]
    }
   ],
   "source": [
    "num_ages = np.unique(df_train['age'].values).shape[0]\n",
    "print(f'Number of age labels: {num_ages}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram = df_train['age'].hist(bins=df_train['age'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram = df_test['age'].hist(bins=df_test['age'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAge(Dataset):\n",
    "    \"\"\"Custom Dataset for loading images\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.csv_path = csv_path\n",
    "        self.df = df\n",
    "        self.y = df['age'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.df.iloc[index]['filename']).convert('L')\n",
    "        img = img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self.y[index]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = 'training_set.csv'\n",
    "TEST_CSV_PATH = 'test_set.csv'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = torch.arange(0, df_train.shape[0]-1000).numpy()\n",
    "valid_indices = torch.arange(df_train.shape[0]-1000, df_train.shape[0]).numpy()\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = DatasetAge(csv_path=TRAIN_CSV_PATH, transform=transform)\n",
    "\n",
    "valid_dataset = DatasetAge(csv_path=TRAIN_CSV_PATH, transform=transform)\n",
    "\n",
    "test_dataset = DatasetAge(csv_path=TEST_CSV_PATH, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4,\n",
    "                          sampler=train_sampler)\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4,\n",
    "                          sampler=valid_sampler)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "inputDim = 250*250\n",
    "outputDim = 121\n",
    "learningRate = 0.001 \n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae_mse_acc(model, data_loader):\n",
    "    mae, mse, acc, num_examples = 0., 0., 0, 0\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "\n",
    "        images = Variable(images.view(-1, inputDim))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        num_examples += labels.size(0)\n",
    "        acc += torch.sum(predicted == labels)\n",
    "        mae += torch.sum(torch.abs(predicted - labels))\n",
    "        mse += torch.sum((predicted - labels)**2)\n",
    "    acc = acc.float() / num_examples   \n",
    "    mae = mae.float() / num_examples\n",
    "    mse = mse.float() / num_examples\n",
    "    return mae, mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/100 | Batch 400/2324 | Loss: 20.9186\n",
      "Epoch: 001/100 | Batch 800/2324 | Loss: 14.0802\n",
      "Epoch: 001/100 | Batch 1200/2324 | Loss: 13.5779\n",
      "Epoch: 001/100 | Batch 1600/2324 | Loss: 22.2548\n",
      "Epoch: 001/100 | Batch 2000/2324 | Loss: 25.1380\n",
      "Epoch: 001/100 | Batch 2324/2324 | Loss: 16.8950\n",
      "Epoch: 001/100\n",
      "Train MAE: 12.33 | Validation MAE: 12.37\n",
      "Time elapsed: 14.06 min\n",
      "Epoch: 002/100 | Batch 400/2324 | Loss: 12.1389\n",
      "Epoch: 002/100 | Batch 800/2324 | Loss: 12.8247\n",
      "Epoch: 002/100 | Batch 1200/2324 | Loss: 10.7119\n",
      "Epoch: 002/100 | Batch 1600/2324 | Loss: 17.5610\n",
      "Epoch: 002/100 | Batch 2000/2324 | Loss: 13.6182\n",
      "Epoch: 002/100 | Batch 2324/2324 | Loss: 14.2620\n",
      "Epoch: 002/100\n",
      "Train MAE: 13.16 | Validation MAE: 13.30\n",
      "Time elapsed: 13.88 min\n",
      "Epoch: 003/100 | Batch 400/2324 | Loss: 12.5524\n",
      "Epoch: 003/100 | Batch 800/2324 | Loss: 19.9387\n",
      "Epoch: 003/100 | Batch 1200/2324 | Loss: 15.6395\n",
      "Epoch: 003/100 | Batch 1600/2324 | Loss: 19.3901\n",
      "Epoch: 003/100 | Batch 2000/2324 | Loss: 17.5174\n",
      "Epoch: 003/100 | Batch 2324/2324 | Loss: 17.7728\n",
      "Epoch: 003/100\n",
      "Train MAE: 12.13 | Validation MAE: 12.84\n",
      "Time elapsed: 14.05 min\n",
      "Epoch: 004/100 | Batch 400/2324 | Loss: 11.8811\n",
      "Epoch: 004/100 | Batch 800/2324 | Loss: 23.2454\n",
      "Epoch: 004/100 | Batch 1200/2324 | Loss: 12.4305\n",
      "Epoch: 004/100 | Batch 1600/2324 | Loss: 28.1801\n",
      "Epoch: 004/100 | Batch 2000/2324 | Loss: 14.1479\n",
      "Epoch: 004/100 | Batch 2324/2324 | Loss: 17.4894\n",
      "Epoch: 004/100\n",
      "Train MAE: 12.71 | Validation MAE: 12.91\n",
      "Time elapsed: 14.27 min\n",
      "Epoch: 005/100 | Batch 400/2324 | Loss: 17.1287\n",
      "Epoch: 005/100 | Batch 800/2324 | Loss: 14.2483\n",
      "Epoch: 005/100 | Batch 1200/2324 | Loss: 14.6054\n",
      "Epoch: 005/100 | Batch 1600/2324 | Loss: 13.2857\n",
      "Epoch: 005/100 | Batch 2000/2324 | Loss: 15.2612\n",
      "Epoch: 005/100 | Batch 2324/2324 | Loss: 15.4163\n",
      "Epoch: 005/100\n",
      "Train MAE: 11.06 | Validation MAE: 11.63\n",
      "Time elapsed: 13.77 min\n",
      "Epoch: 006/100 | Batch 400/2324 | Loss: 14.1522\n",
      "Epoch: 006/100 | Batch 800/2324 | Loss: 15.2946\n",
      "Epoch: 006/100 | Batch 1200/2324 | Loss: 15.5337\n",
      "Epoch: 006/100 | Batch 1600/2324 | Loss: 13.7196\n",
      "Epoch: 006/100 | Batch 2000/2324 | Loss: 13.3896\n",
      "Epoch: 006/100 | Batch 2324/2324 | Loss: 14.2671\n",
      "Epoch: 006/100\n",
      "Train MAE: 13.58 | Validation MAE: 15.05\n",
      "Time elapsed: 15.10 min\n",
      "Epoch: 007/100 | Batch 400/2324 | Loss: 10.3424\n",
      "Epoch: 007/100 | Batch 800/2324 | Loss: 14.7755\n",
      "Epoch: 007/100 | Batch 1200/2324 | Loss: 19.0383\n",
      "Epoch: 007/100 | Batch 1600/2324 | Loss: 16.0646\n",
      "Epoch: 007/100 | Batch 2000/2324 | Loss: 19.9092\n",
      "Epoch: 007/100 | Batch 2324/2324 | Loss: 13.2188\n",
      "Epoch: 007/100\n",
      "Train MAE: 12.63 | Validation MAE: 13.59\n",
      "Time elapsed: 15.08 min\n",
      "Epoch: 008/100 | Batch 400/2324 | Loss: 10.2982\n",
      "Epoch: 008/100 | Batch 800/2324 | Loss: 16.6591\n",
      "Epoch: 008/100 | Batch 1200/2324 | Loss: 11.1658\n",
      "Epoch: 008/100 | Batch 1600/2324 | Loss: 11.3035\n",
      "Epoch: 008/100 | Batch 2000/2324 | Loss: 17.8368\n",
      "Epoch: 008/100 | Batch 2324/2324 | Loss: 11.7068\n",
      "Epoch: 008/100\n",
      "Train MAE: 11.40 | Validation MAE: 12.48\n",
      "Time elapsed: 14.40 min\n",
      "Epoch: 009/100 | Batch 400/2324 | Loss: 12.3164\n",
      "Epoch: 009/100 | Batch 800/2324 | Loss: 18.1061\n",
      "Epoch: 009/100 | Batch 1200/2324 | Loss: 16.4621\n",
      "Epoch: 009/100 | Batch 1600/2324 | Loss: 9.8504\n",
      "Epoch: 009/100 | Batch 2000/2324 | Loss: 12.4854\n",
      "Epoch: 009/100 | Batch 2324/2324 | Loss: 13.4845\n",
      "Epoch: 009/100\n",
      "Train MAE: 11.08 | Validation MAE: 12.25\n",
      "Time elapsed: 15.24 min\n",
      "Epoch: 010/100 | Batch 400/2324 | Loss: 8.8317\n",
      "Epoch: 010/100 | Batch 800/2324 | Loss: 12.8312\n",
      "Epoch: 010/100 | Batch 1200/2324 | Loss: 14.8487\n",
      "Epoch: 010/100 | Batch 1600/2324 | Loss: 8.2853\n",
      "Epoch: 010/100 | Batch 2000/2324 | Loss: 19.3680\n",
      "Epoch: 010/100 | Batch 2324/2324 | Loss: 16.5578\n",
      "Epoch: 010/100\n",
      "Train MAE: 11.28 | Validation MAE: 12.52\n",
      "Time elapsed: 15.36 min\n",
      "Epoch: 011/100 | Batch 400/2324 | Loss: 20.2532\n",
      "Epoch: 011/100 | Batch 800/2324 | Loss: 10.4118\n",
      "Epoch: 011/100 | Batch 1200/2324 | Loss: 11.4714\n",
      "Epoch: 011/100 | Batch 1600/2324 | Loss: 12.1887\n",
      "Epoch: 011/100 | Batch 2000/2324 | Loss: 18.2701\n",
      "Epoch: 011/100 | Batch 2324/2324 | Loss: 13.8366\n",
      "Epoch: 011/100\n",
      "Train MAE: 10.44 | Validation MAE: 12.39\n",
      "Time elapsed: 15.25 min\n",
      "Epoch: 012/100 | Batch 400/2324 | Loss: 13.5644\n",
      "Epoch: 012/100 | Batch 800/2324 | Loss: 10.8125\n",
      "Epoch: 012/100 | Batch 1200/2324 | Loss: 17.4339\n",
      "Epoch: 012/100 | Batch 1600/2324 | Loss: 15.5056\n",
      "Epoch: 012/100 | Batch 2000/2324 | Loss: 11.7879\n",
      "Epoch: 012/100 | Batch 2324/2324 | Loss: 23.4109\n",
      "Epoch: 012/100\n",
      "Train MAE: 10.70 | Validation MAE: 12.04\n",
      "Time elapsed: 18.07 min\n",
      "Epoch: 013/100 | Batch 400/2324 | Loss: 15.9238\n",
      "Epoch: 013/100 | Batch 800/2324 | Loss: 10.9116\n",
      "Epoch: 013/100 | Batch 1200/2324 | Loss: 18.9588\n",
      "Epoch: 013/100 | Batch 1600/2324 | Loss: 12.3456\n",
      "Epoch: 013/100 | Batch 2000/2324 | Loss: 25.1681\n",
      "Epoch: 013/100 | Batch 2324/2324 | Loss: 9.6624\n",
      "Epoch: 013/100\n",
      "Train MAE: 11.78 | Validation MAE: 13.28\n",
      "Time elapsed: 26.80 min\n",
      "Epoch: 014/100 | Batch 400/2324 | Loss: 14.2306\n",
      "Epoch: 014/100 | Batch 800/2324 | Loss: 12.2874\n",
      "Epoch: 014/100 | Batch 1200/2324 | Loss: 12.3133\n",
      "Epoch: 014/100 | Batch 1600/2324 | Loss: 9.3585\n",
      "Epoch: 014/100 | Batch 2000/2324 | Loss: 10.7552\n",
      "Epoch: 014/100 | Batch 2324/2324 | Loss: 18.0034\n",
      "Epoch: 014/100\n",
      "Train MAE: 12.21 | Validation MAE: 13.31\n",
      "Time elapsed: 13.68 min\n",
      "Epoch: 015/100 | Batch 400/2324 | Loss: 12.1237\n",
      "Epoch: 015/100 | Batch 800/2324 | Loss: 7.6779\n",
      "Epoch: 015/100 | Batch 1200/2324 | Loss: 13.0086\n",
      "Epoch: 015/100 | Batch 1600/2324 | Loss: 13.0515\n",
      "Epoch: 015/100 | Batch 2000/2324 | Loss: 11.6758\n",
      "Epoch: 015/100 | Batch 2324/2324 | Loss: 11.2577\n",
      "Epoch: 015/100\n",
      "Train MAE: 11.17 | Validation MAE: 13.46\n",
      "Time elapsed: 14.11 min\n",
      "Epoch: 016/100 | Batch 400/2324 | Loss: 20.5911\n",
      "Epoch: 016/100 | Batch 800/2324 | Loss: 14.5933\n",
      "Epoch: 016/100 | Batch 1200/2324 | Loss: 11.0661\n",
      "Epoch: 016/100 | Batch 1600/2324 | Loss: 7.9874\n",
      "Epoch: 016/100 | Batch 2000/2324 | Loss: 10.8728\n",
      "Epoch: 016/100 | Batch 2324/2324 | Loss: 13.3139\n",
      "Epoch: 016/100\n",
      "Train MAE: 10.54 | Validation MAE: 12.61\n",
      "Time elapsed: 13.54 min\n",
      "Epoch: 017/100 | Batch 400/2324 | Loss: 10.3611\n",
      "Epoch: 017/100 | Batch 800/2324 | Loss: 8.3062\n",
      "Epoch: 017/100 | Batch 1200/2324 | Loss: 9.6301\n",
      "Epoch: 017/100 | Batch 1600/2324 | Loss: 14.3344\n",
      "Epoch: 017/100 | Batch 2000/2324 | Loss: 17.8646\n",
      "Epoch: 017/100 | Batch 2324/2324 | Loss: 12.1327\n",
      "Epoch: 017/100\n",
      "Train MAE: 11.39 | Validation MAE: 12.93\n",
      "Time elapsed: 13.36 min\n",
      "Epoch: 018/100 | Batch 400/2324 | Loss: 16.4416\n",
      "Epoch: 018/100 | Batch 800/2324 | Loss: 9.1340\n",
      "Epoch: 018/100 | Batch 1200/2324 | Loss: 13.4380\n",
      "Epoch: 018/100 | Batch 1600/2324 | Loss: 20.5030\n",
      "Epoch: 018/100 | Batch 2000/2324 | Loss: 13.3756\n",
      "Epoch: 018/100 | Batch 2324/2324 | Loss: 9.3355\n",
      "Epoch: 018/100\n",
      "Train MAE: 10.53 | Validation MAE: 12.62\n",
      "Time elapsed: 13.65 min\n",
      "Epoch: 019/100 | Batch 400/2324 | Loss: 9.6750\n",
      "Epoch: 019/100 | Batch 800/2324 | Loss: 10.3746\n",
      "Epoch: 019/100 | Batch 1200/2324 | Loss: 9.0024\n",
      "Epoch: 019/100 | Batch 1600/2324 | Loss: 8.7381\n",
      "Epoch: 019/100 | Batch 2000/2324 | Loss: 10.8874\n",
      "Epoch: 019/100 | Batch 2324/2324 | Loss: 11.0804\n",
      "Epoch: 019/100\n",
      "Train MAE: 9.96 | Validation MAE: 12.16\n",
      "Time elapsed: 14.79 min\n",
      "Epoch: 020/100 | Batch 400/2324 | Loss: 8.4434\n",
      "Epoch: 020/100 | Batch 800/2324 | Loss: 18.5132\n",
      "Epoch: 020/100 | Batch 1200/2324 | Loss: 12.0136\n",
      "Epoch: 020/100 | Batch 1600/2324 | Loss: 4.4921\n",
      "Epoch: 020/100 | Batch 2000/2324 | Loss: 9.9875\n",
      "Epoch: 020/100 | Batch 2324/2324 | Loss: 10.8592\n",
      "Epoch: 020/100\n",
      "Train MAE: 11.61 | Validation MAE: 13.81\n",
      "Time elapsed: 14.01 min\n",
      "Epoch: 021/100 | Batch 400/2324 | Loss: 13.4723\n",
      "Epoch: 021/100 | Batch 800/2324 | Loss: 11.4289\n",
      "Epoch: 021/100 | Batch 1200/2324 | Loss: 14.9391\n",
      "Epoch: 021/100 | Batch 1600/2324 | Loss: 9.4469\n",
      "Epoch: 021/100 | Batch 2000/2324 | Loss: 16.9633\n",
      "Epoch: 021/100 | Batch 2324/2324 | Loss: 8.7060\n",
      "Epoch: 021/100\n",
      "Train MAE: 11.36 | Validation MAE: 14.21\n",
      "Time elapsed: 13.02 min\n",
      "Epoch: 022/100 | Batch 400/2324 | Loss: 16.2034\n",
      "Epoch: 022/100 | Batch 800/2324 | Loss: 12.7647\n",
      "Epoch: 022/100 | Batch 1200/2324 | Loss: 9.2033\n",
      "Epoch: 022/100 | Batch 1600/2324 | Loss: 14.8715\n",
      "Epoch: 022/100 | Batch 2000/2324 | Loss: 9.8134\n",
      "Epoch: 022/100 | Batch 2324/2324 | Loss: 12.9597\n",
      "Epoch: 022/100\n",
      "Train MAE: 9.68 | Validation MAE: 12.56\n",
      "Time elapsed: 14.45 min\n",
      "Epoch: 023/100 | Batch 400/2324 | Loss: 11.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023/100 | Batch 800/2324 | Loss: 9.6691\n",
      "Epoch: 023/100 | Batch 1200/2324 | Loss: 9.9456\n",
      "Epoch: 023/100 | Batch 1600/2324 | Loss: 7.4722\n",
      "Epoch: 023/100 | Batch 2000/2324 | Loss: 6.7588\n",
      "Epoch: 023/100 | Batch 2324/2324 | Loss: 7.6405\n",
      "Epoch: 023/100\n",
      "Train MAE: 10.45 | Validation MAE: 12.95\n",
      "Time elapsed: 13.11 min\n",
      "Epoch: 024/100 | Batch 400/2324 | Loss: 7.3347\n",
      "Epoch: 024/100 | Batch 800/2324 | Loss: 10.9734\n",
      "Epoch: 024/100 | Batch 1200/2324 | Loss: 13.9965\n",
      "Epoch: 024/100 | Batch 1600/2324 | Loss: 7.5572\n",
      "Epoch: 024/100 | Batch 2000/2324 | Loss: 8.3124\n",
      "Epoch: 024/100 | Batch 2324/2324 | Loss: 7.8691\n",
      "Epoch: 024/100\n",
      "Train MAE: 10.00 | Validation MAE: 11.98\n",
      "Time elapsed: 13.26 min\n",
      "Epoch: 025/100 | Batch 400/2324 | Loss: 16.0641\n",
      "Epoch: 025/100 | Batch 800/2324 | Loss: 8.7437\n",
      "Epoch: 025/100 | Batch 1200/2324 | Loss: 12.8265\n",
      "Epoch: 025/100 | Batch 1600/2324 | Loss: 12.4726\n",
      "Epoch: 025/100 | Batch 2000/2324 | Loss: 11.1537\n",
      "Epoch: 025/100 | Batch 2324/2324 | Loss: 7.7401\n",
      "Epoch: 025/100\n",
      "Train MAE: 10.06 | Validation MAE: 12.92\n",
      "Time elapsed: 12.80 min\n",
      "Epoch: 026/100 | Batch 400/2324 | Loss: 7.7064\n",
      "Epoch: 026/100 | Batch 800/2324 | Loss: 10.5676\n",
      "Epoch: 026/100 | Batch 1200/2324 | Loss: 5.2481\n",
      "Epoch: 026/100 | Batch 1600/2324 | Loss: 9.4119\n",
      "Epoch: 026/100 | Batch 2000/2324 | Loss: 13.0124\n",
      "Epoch: 026/100 | Batch 2324/2324 | Loss: 9.9871\n",
      "Epoch: 026/100\n",
      "Train MAE: 8.66 | Validation MAE: 11.52\n",
      "Time elapsed: 13.29 min\n",
      "Epoch: 027/100 | Batch 400/2324 | Loss: 8.3834\n",
      "Epoch: 027/100 | Batch 800/2324 | Loss: 7.3069\n",
      "Epoch: 027/100 | Batch 1200/2324 | Loss: 9.4962\n",
      "Epoch: 027/100 | Batch 1600/2324 | Loss: 8.8146\n",
      "Epoch: 027/100 | Batch 2000/2324 | Loss: 11.4583\n",
      "Epoch: 027/100 | Batch 2324/2324 | Loss: 7.8826\n",
      "Epoch: 027/100\n",
      "Train MAE: 8.94 | Validation MAE: 12.43\n",
      "Time elapsed: 19.73 min\n",
      "Epoch: 028/100 | Batch 400/2324 | Loss: 7.0230\n",
      "Epoch: 028/100 | Batch 800/2324 | Loss: 10.9610\n",
      "Epoch: 028/100 | Batch 1200/2324 | Loss: 13.2734\n",
      "Epoch: 028/100 | Batch 1600/2324 | Loss: 10.9423\n",
      "Epoch: 028/100 | Batch 2000/2324 | Loss: 13.5178\n",
      "Epoch: 028/100 | Batch 2324/2324 | Loss: 7.5337\n",
      "Epoch: 028/100\n",
      "Train MAE: 9.17 | Validation MAE: 12.52\n",
      "Time elapsed: 14.61 min\n",
      "Epoch: 029/100 | Batch 400/2324 | Loss: 11.1834\n",
      "Epoch: 029/100 | Batch 800/2324 | Loss: 10.7099\n",
      "Epoch: 029/100 | Batch 1200/2324 | Loss: 10.2002\n",
      "Epoch: 029/100 | Batch 1600/2324 | Loss: 14.1445\n",
      "Epoch: 029/100 | Batch 2000/2324 | Loss: 6.6827\n",
      "Epoch: 029/100 | Batch 2324/2324 | Loss: 20.4623\n",
      "Epoch: 029/100\n",
      "Train MAE: 9.30 | Validation MAE: 11.95\n",
      "Time elapsed: 13.29 min\n",
      "Epoch: 030/100 | Batch 400/2324 | Loss: 9.4986\n",
      "Epoch: 030/100 | Batch 800/2324 | Loss: 12.9526\n",
      "Epoch: 030/100 | Batch 1200/2324 | Loss: 9.9569\n",
      "Epoch: 030/100 | Batch 1600/2324 | Loss: 7.1734\n",
      "Epoch: 030/100 | Batch 2000/2324 | Loss: 7.5122\n",
      "Epoch: 030/100 | Batch 2324/2324 | Loss: 7.8892\n",
      "Epoch: 030/100\n",
      "Train MAE: 8.93 | Validation MAE: 11.82\n",
      "Time elapsed: 14.61 min\n",
      "Epoch: 031/100 | Batch 400/2324 | Loss: 10.1979\n",
      "Epoch: 031/100 | Batch 800/2324 | Loss: 9.6407\n",
      "Epoch: 031/100 | Batch 1200/2324 | Loss: 13.5266\n",
      "Epoch: 031/100 | Batch 1600/2324 | Loss: 8.8041\n",
      "Epoch: 031/100 | Batch 2000/2324 | Loss: 9.3607\n",
      "Epoch: 031/100 | Batch 2324/2324 | Loss: 16.5971\n",
      "Epoch: 031/100\n",
      "Train MAE: 9.42 | Validation MAE: 12.48\n",
      "Time elapsed: 15.11 min\n",
      "Epoch: 032/100 | Batch 400/2324 | Loss: 8.2916\n",
      "Epoch: 032/100 | Batch 800/2324 | Loss: 14.1749\n",
      "Epoch: 032/100 | Batch 1200/2324 | Loss: 7.7657\n",
      "Epoch: 032/100 | Batch 1600/2324 | Loss: 8.4040\n",
      "Epoch: 032/100 | Batch 2000/2324 | Loss: 7.8648\n",
      "Epoch: 032/100 | Batch 2324/2324 | Loss: 5.0934\n",
      "Epoch: 032/100\n",
      "Train MAE: 9.43 | Validation MAE: 12.33\n",
      "Time elapsed: 15.52 min\n",
      "Epoch: 033/100 | Batch 400/2324 | Loss: 8.9572\n",
      "Epoch: 033/100 | Batch 800/2324 | Loss: 5.5897\n",
      "Epoch: 033/100 | Batch 1200/2324 | Loss: 8.4370\n",
      "Epoch: 033/100 | Batch 1600/2324 | Loss: 15.2115\n",
      "Epoch: 033/100 | Batch 2000/2324 | Loss: 7.5838\n",
      "Epoch: 033/100 | Batch 2324/2324 | Loss: 10.1990\n",
      "Epoch: 033/100\n",
      "Train MAE: 8.84 | Validation MAE: 12.22\n",
      "Time elapsed: 12.65 min\n",
      "Epoch: 034/100 | Batch 400/2324 | Loss: 14.9668\n",
      "Epoch: 034/100 | Batch 800/2324 | Loss: 6.2920\n",
      "Epoch: 034/100 | Batch 1200/2324 | Loss: 9.9443\n",
      "Epoch: 034/100 | Batch 1600/2324 | Loss: 16.2659\n",
      "Epoch: 034/100 | Batch 2000/2324 | Loss: 6.3138\n",
      "Epoch: 034/100 | Batch 2324/2324 | Loss: 7.0498\n",
      "Epoch: 034/100\n",
      "Train MAE: 8.75 | Validation MAE: 11.87\n",
      "Time elapsed: 25.57 min\n",
      "Epoch: 035/100 | Batch 400/2324 | Loss: 8.5874\n",
      "Epoch: 035/100 | Batch 800/2324 | Loss: 10.8172\n",
      "Epoch: 035/100 | Batch 1200/2324 | Loss: 10.7585\n",
      "Epoch: 035/100 | Batch 1600/2324 | Loss: 15.9843\n",
      "Epoch: 035/100 | Batch 2000/2324 | Loss: 6.4009\n",
      "Epoch: 035/100 | Batch 2324/2324 | Loss: 9.8762\n",
      "Epoch: 035/100\n",
      "Train MAE: 9.51 | Validation MAE: 12.59\n",
      "Time elapsed: 11.99 min\n",
      "Epoch: 036/100 | Batch 400/2324 | Loss: 6.2188\n",
      "Epoch: 036/100 | Batch 800/2324 | Loss: 8.3965\n",
      "Epoch: 036/100 | Batch 1200/2324 | Loss: 7.8201\n",
      "Epoch: 036/100 | Batch 1600/2324 | Loss: 11.9258\n",
      "Epoch: 036/100 | Batch 2000/2324 | Loss: 11.4153\n",
      "Epoch: 036/100 | Batch 2324/2324 | Loss: 15.8229\n",
      "Epoch: 036/100\n",
      "Train MAE: 8.19 | Validation MAE: 11.13\n",
      "Time elapsed: 11.89 min\n",
      "Epoch: 037/100 | Batch 400/2324 | Loss: 8.2827\n",
      "Epoch: 037/100 | Batch 800/2324 | Loss: 8.8407\n",
      "Epoch: 037/100 | Batch 1200/2324 | Loss: 13.4564\n",
      "Epoch: 037/100 | Batch 1600/2324 | Loss: 5.1374\n",
      "Epoch: 037/100 | Batch 2000/2324 | Loss: 12.1897\n",
      "Epoch: 037/100 | Batch 2324/2324 | Loss: 5.6997\n",
      "Epoch: 037/100\n",
      "Train MAE: 9.03 | Validation MAE: 12.34\n",
      "Time elapsed: 11.92 min\n",
      "Epoch: 038/100 | Batch 400/2324 | Loss: 8.7500\n",
      "Epoch: 038/100 | Batch 800/2324 | Loss: 8.5964\n",
      "Epoch: 038/100 | Batch 1200/2324 | Loss: 4.6677\n",
      "Epoch: 038/100 | Batch 1600/2324 | Loss: 9.2301\n",
      "Epoch: 038/100 | Batch 2000/2324 | Loss: 5.2765\n",
      "Epoch: 038/100 | Batch 2324/2324 | Loss: 10.2363\n",
      "Epoch: 038/100\n",
      "Train MAE: 9.45 | Validation MAE: 13.08\n",
      "Time elapsed: 11.90 min\n",
      "Epoch: 039/100 | Batch 400/2324 | Loss: 8.2949\n",
      "Epoch: 039/100 | Batch 800/2324 | Loss: 9.8718\n",
      "Epoch: 039/100 | Batch 1200/2324 | Loss: 7.2998\n",
      "Epoch: 039/100 | Batch 1600/2324 | Loss: 11.6748\n",
      "Epoch: 039/100 | Batch 2000/2324 | Loss: 10.3440\n",
      "Epoch: 039/100 | Batch 2324/2324 | Loss: 6.9993\n",
      "Epoch: 039/100\n",
      "Train MAE: 8.42 | Validation MAE: 11.91\n",
      "Time elapsed: 11.81 min\n",
      "Epoch: 040/100 | Batch 400/2324 | Loss: 11.3247\n",
      "Epoch: 040/100 | Batch 800/2324 | Loss: 8.9903\n",
      "Epoch: 040/100 | Batch 1200/2324 | Loss: 7.2832\n",
      "Epoch: 040/100 | Batch 1600/2324 | Loss: 6.2605\n",
      "Epoch: 040/100 | Batch 2000/2324 | Loss: 9.7987\n",
      "Epoch: 040/100 | Batch 2324/2324 | Loss: 6.1301\n",
      "Epoch: 040/100\n",
      "Train MAE: 8.74 | Validation MAE: 12.92\n",
      "Time elapsed: 11.68 min\n",
      "Epoch: 041/100 | Batch 400/2324 | Loss: 5.4946\n",
      "Epoch: 041/100 | Batch 800/2324 | Loss: 13.0366\n",
      "Epoch: 041/100 | Batch 1200/2324 | Loss: 14.2346\n",
      "Epoch: 041/100 | Batch 1600/2324 | Loss: 13.9665\n",
      "Epoch: 041/100 | Batch 2000/2324 | Loss: 7.0691\n",
      "Epoch: 041/100 | Batch 2324/2324 | Loss: 15.8219\n",
      "Epoch: 041/100\n",
      "Train MAE: 9.06 | Validation MAE: 11.87\n",
      "Time elapsed: 11.41 min\n",
      "Epoch: 042/100 | Batch 400/2324 | Loss: 11.4912\n",
      "Epoch: 042/100 | Batch 800/2324 | Loss: 6.6162\n",
      "Epoch: 042/100 | Batch 1200/2324 | Loss: 7.0011\n",
      "Epoch: 042/100 | Batch 1600/2324 | Loss: 6.7047\n",
      "Epoch: 042/100 | Batch 2000/2324 | Loss: 10.9417\n",
      "Epoch: 042/100 | Batch 2324/2324 | Loss: 7.4496\n",
      "Epoch: 042/100\n",
      "Train MAE: 9.70 | Validation MAE: 13.30\n",
      "Time elapsed: 11.46 min\n",
      "Epoch: 043/100 | Batch 400/2324 | Loss: 16.8072\n",
      "Epoch: 043/100 | Batch 800/2324 | Loss: 6.4114\n",
      "Epoch: 043/100 | Batch 1200/2324 | Loss: 9.5540\n",
      "Epoch: 043/100 | Batch 1600/2324 | Loss: 9.4831\n",
      "Epoch: 043/100 | Batch 2000/2324 | Loss: 12.9295\n",
      "Epoch: 043/100 | Batch 2324/2324 | Loss: 9.1301\n",
      "Epoch: 043/100\n",
      "Train MAE: 7.98 | Validation MAE: 11.89\n",
      "Time elapsed: 11.48 min\n",
      "Epoch: 044/100 | Batch 400/2324 | Loss: 8.3037\n",
      "Epoch: 044/100 | Batch 800/2324 | Loss: 8.8647\n",
      "Epoch: 044/100 | Batch 1200/2324 | Loss: 14.3175\n",
      "Epoch: 044/100 | Batch 1600/2324 | Loss: 6.8497\n",
      "Epoch: 044/100 | Batch 2000/2324 | Loss: 17.1581\n",
      "Epoch: 044/100 | Batch 2324/2324 | Loss: 24.5362\n",
      "Epoch: 044/100\n",
      "Train MAE: 8.40 | Validation MAE: 12.91\n",
      "Time elapsed: 11.43 min\n",
      "Epoch: 045/100 | Batch 400/2324 | Loss: 7.2752\n",
      "Epoch: 045/100 | Batch 800/2324 | Loss: 5.4195\n",
      "Epoch: 045/100 | Batch 1200/2324 | Loss: 8.8557\n",
      "Epoch: 045/100 | Batch 1600/2324 | Loss: 4.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 045/100 | Batch 2000/2324 | Loss: 10.3052\n",
      "Epoch: 045/100 | Batch 2324/2324 | Loss: 6.8307\n",
      "Epoch: 045/100\n",
      "Train MAE: 8.03 | Validation MAE: 11.67\n",
      "Time elapsed: 11.39 min\n",
      "Epoch: 046/100 | Batch 400/2324 | Loss: 7.0504\n",
      "Epoch: 046/100 | Batch 800/2324 | Loss: 5.7360\n",
      "Epoch: 046/100 | Batch 1200/2324 | Loss: 15.3221\n",
      "Epoch: 046/100 | Batch 1600/2324 | Loss: 9.7190\n",
      "Epoch: 046/100 | Batch 2000/2324 | Loss: 6.9206\n",
      "Epoch: 046/100 | Batch 2324/2324 | Loss: 12.0448\n",
      "Epoch: 046/100\n",
      "Train MAE: 8.47 | Validation MAE: 11.84\n",
      "Time elapsed: 11.42 min\n",
      "Epoch: 047/100 | Batch 400/2324 | Loss: 9.8999\n",
      "Epoch: 047/100 | Batch 800/2324 | Loss: 6.0137\n",
      "Epoch: 047/100 | Batch 1200/2324 | Loss: 5.8443\n",
      "Epoch: 047/100 | Batch 1600/2324 | Loss: 10.1439\n",
      "Epoch: 047/100 | Batch 2000/2324 | Loss: 5.6750\n",
      "Epoch: 047/100 | Batch 2324/2324 | Loss: 10.3230\n",
      "Epoch: 047/100\n",
      "Train MAE: 8.70 | Validation MAE: 13.07\n",
      "Time elapsed: 11.44 min\n",
      "Epoch: 048/100 | Batch 400/2324 | Loss: 6.8086\n",
      "Epoch: 048/100 | Batch 800/2324 | Loss: 6.0135\n",
      "Epoch: 048/100 | Batch 1200/2324 | Loss: 10.5179\n",
      "Epoch: 048/100 | Batch 1600/2324 | Loss: 4.5194\n",
      "Epoch: 048/100 | Batch 2000/2324 | Loss: 7.2603\n",
      "Epoch: 048/100 | Batch 2324/2324 | Loss: 8.5007\n",
      "Epoch: 048/100\n",
      "Train MAE: 8.46 | Validation MAE: 11.65\n",
      "Time elapsed: 11.34 min\n",
      "Epoch: 049/100 | Batch 400/2324 | Loss: 4.2450\n",
      "Epoch: 049/100 | Batch 800/2324 | Loss: 11.5322\n",
      "Epoch: 049/100 | Batch 1200/2324 | Loss: 8.0372\n",
      "Epoch: 049/100 | Batch 1600/2324 | Loss: 6.9570\n",
      "Epoch: 049/100 | Batch 2000/2324 | Loss: 7.5178\n",
      "Epoch: 049/100 | Batch 2324/2324 | Loss: 11.1871\n",
      "Epoch: 049/100\n",
      "Train MAE: 9.29 | Validation MAE: 12.25\n",
      "Time elapsed: 11.42 min\n",
      "Epoch: 050/100 | Batch 400/2324 | Loss: 6.4873\n",
      "Epoch: 050/100 | Batch 800/2324 | Loss: 5.5604\n",
      "Epoch: 050/100 | Batch 1200/2324 | Loss: 6.8121\n",
      "Epoch: 050/100 | Batch 1600/2324 | Loss: 6.4222\n",
      "Epoch: 050/100 | Batch 2000/2324 | Loss: 7.2093\n",
      "Epoch: 050/100 | Batch 2324/2324 | Loss: 7.2020\n",
      "Epoch: 050/100\n",
      "Train MAE: 8.66 | Validation MAE: 12.00\n",
      "Time elapsed: 11.35 min\n",
      "Epoch: 051/100 | Batch 400/2324 | Loss: 6.5756\n",
      "Epoch: 051/100 | Batch 800/2324 | Loss: 5.7629\n",
      "Epoch: 051/100 | Batch 1200/2324 | Loss: 6.2367\n",
      "Epoch: 051/100 | Batch 1600/2324 | Loss: 14.1656\n",
      "Epoch: 051/100 | Batch 2000/2324 | Loss: 6.6955\n",
      "Epoch: 051/100 | Batch 2324/2324 | Loss: 5.0298\n",
      "Epoch: 051/100\n",
      "Train MAE: 8.41 | Validation MAE: 13.09\n",
      "Time elapsed: 11.39 min\n",
      "Epoch: 052/100 | Batch 400/2324 | Loss: 5.5056\n",
      "Epoch: 052/100 | Batch 800/2324 | Loss: 3.8700\n",
      "Epoch: 052/100 | Batch 1200/2324 | Loss: 11.4026\n",
      "Epoch: 052/100 | Batch 1600/2324 | Loss: 7.5943\n",
      "Epoch: 052/100 | Batch 2000/2324 | Loss: 6.4347\n",
      "Epoch: 052/100 | Batch 2324/2324 | Loss: 5.6810\n",
      "Epoch: 052/100\n",
      "Train MAE: 7.41 | Validation MAE: 12.02\n",
      "Time elapsed: 11.33 min\n",
      "Epoch: 053/100 | Batch 400/2324 | Loss: 8.9591\n",
      "Epoch: 053/100 | Batch 800/2324 | Loss: 5.6891\n",
      "Epoch: 053/100 | Batch 1200/2324 | Loss: 5.1606\n",
      "Epoch: 053/100 | Batch 1600/2324 | Loss: 6.2280\n",
      "Epoch: 053/100 | Batch 2000/2324 | Loss: 7.8594\n",
      "Epoch: 053/100 | Batch 2324/2324 | Loss: 6.0487\n",
      "Epoch: 053/100\n",
      "Train MAE: 7.08 | Validation MAE: 11.74\n",
      "Time elapsed: 11.37 min\n",
      "Epoch: 054/100 | Batch 400/2324 | Loss: 11.5705\n",
      "Epoch: 054/100 | Batch 800/2324 | Loss: 9.1303\n",
      "Epoch: 054/100 | Batch 1200/2324 | Loss: 6.7651\n",
      "Epoch: 054/100 | Batch 1600/2324 | Loss: 8.1378\n",
      "Epoch: 054/100 | Batch 2000/2324 | Loss: 6.0527\n",
      "Epoch: 054/100 | Batch 2324/2324 | Loss: 9.6224\n",
      "Epoch: 054/100\n",
      "Train MAE: 7.16 | Validation MAE: 12.23\n",
      "Time elapsed: 11.35 min\n",
      "Epoch: 055/100 | Batch 400/2324 | Loss: 6.6757\n",
      "Epoch: 055/100 | Batch 800/2324 | Loss: 12.6409\n",
      "Epoch: 055/100 | Batch 1200/2324 | Loss: 6.2381\n",
      "Epoch: 055/100 | Batch 1600/2324 | Loss: 6.4897\n",
      "Epoch: 055/100 | Batch 2000/2324 | Loss: 8.0394\n",
      "Epoch: 055/100 | Batch 2324/2324 | Loss: 9.2250\n",
      "Epoch: 055/100\n",
      "Train MAE: 7.45 | Validation MAE: 12.15\n",
      "Time elapsed: 11.37 min\n",
      "Epoch: 056/100 | Batch 400/2324 | Loss: 11.2747\n",
      "Epoch: 056/100 | Batch 800/2324 | Loss: 12.3434\n",
      "Epoch: 056/100 | Batch 1200/2324 | Loss: 6.8808\n",
      "Epoch: 056/100 | Batch 1600/2324 | Loss: 7.0595\n",
      "Epoch: 056/100 | Batch 2000/2324 | Loss: 12.0487\n",
      "Epoch: 056/100 | Batch 2324/2324 | Loss: 16.5245\n",
      "Epoch: 056/100\n",
      "Train MAE: 7.44 | Validation MAE: 11.90\n",
      "Time elapsed: 11.33 min\n",
      "Epoch: 057/100 | Batch 400/2324 | Loss: 3.2584\n",
      "Epoch: 057/100 | Batch 800/2324 | Loss: 9.9937\n",
      "Epoch: 057/100 | Batch 1200/2324 | Loss: 5.6154\n",
      "Epoch: 057/100 | Batch 1600/2324 | Loss: 5.9659\n",
      "Epoch: 057/100 | Batch 2000/2324 | Loss: 10.7661\n",
      "Epoch: 057/100 | Batch 2324/2324 | Loss: 6.1168\n",
      "Epoch: 057/100\n",
      "Train MAE: 7.13 | Validation MAE: 12.31\n",
      "Time elapsed: 11.32 min\n",
      "Epoch: 058/100 | Batch 400/2324 | Loss: 6.3614\n",
      "Epoch: 058/100 | Batch 800/2324 | Loss: 6.5213\n",
      "Epoch: 058/100 | Batch 1200/2324 | Loss: 8.4342\n",
      "Epoch: 058/100 | Batch 1600/2324 | Loss: 7.0408\n",
      "Epoch: 058/100 | Batch 2000/2324 | Loss: 7.9626\n",
      "Epoch: 058/100 | Batch 2324/2324 | Loss: 9.8136\n",
      "Epoch: 058/100\n",
      "Train MAE: 7.77 | Validation MAE: 11.73\n",
      "Time elapsed: 11.33 min\n",
      "Epoch: 059/100 | Batch 400/2324 | Loss: 4.6359\n",
      "Epoch: 059/100 | Batch 800/2324 | Loss: 10.4043\n",
      "Epoch: 059/100 | Batch 1200/2324 | Loss: 8.4277\n",
      "Epoch: 059/100 | Batch 1600/2324 | Loss: 5.9094\n",
      "Epoch: 059/100 | Batch 2000/2324 | Loss: 7.6369\n",
      "Epoch: 059/100 | Batch 2324/2324 | Loss: 9.3166\n",
      "Epoch: 059/100\n",
      "Train MAE: 7.75 | Validation MAE: 12.14\n",
      "Time elapsed: 11.25 min\n",
      "Epoch: 060/100 | Batch 400/2324 | Loss: 11.8726\n",
      "Epoch: 060/100 | Batch 800/2324 | Loss: 7.0433\n",
      "Epoch: 060/100 | Batch 1200/2324 | Loss: 6.4183\n",
      "Epoch: 060/100 | Batch 1600/2324 | Loss: 5.4208\n",
      "Epoch: 060/100 | Batch 2000/2324 | Loss: 7.0878\n",
      "Epoch: 060/100 | Batch 2324/2324 | Loss: 6.7960\n",
      "Epoch: 060/100\n",
      "Train MAE: 8.67 | Validation MAE: 13.82\n",
      "Time elapsed: 11.29 min\n",
      "Epoch: 061/100 | Batch 400/2324 | Loss: 9.9276\n",
      "Epoch: 061/100 | Batch 800/2324 | Loss: 7.7492\n",
      "Epoch: 061/100 | Batch 1200/2324 | Loss: 5.8259\n",
      "Epoch: 061/100 | Batch 1600/2324 | Loss: 9.4696\n",
      "Epoch: 061/100 | Batch 2000/2324 | Loss: 6.0085\n",
      "Epoch: 061/100 | Batch 2324/2324 | Loss: 8.3979\n",
      "Epoch: 061/100\n",
      "Train MAE: 8.03 | Validation MAE: 12.70\n",
      "Time elapsed: 11.31 min\n",
      "Epoch: 062/100 | Batch 400/2324 | Loss: 6.0373\n",
      "Epoch: 062/100 | Batch 800/2324 | Loss: 4.5186\n",
      "Epoch: 062/100 | Batch 1200/2324 | Loss: 6.1118\n",
      "Epoch: 062/100 | Batch 1600/2324 | Loss: 5.0989\n",
      "Epoch: 062/100 | Batch 2000/2324 | Loss: 5.3218\n",
      "Epoch: 062/100 | Batch 2324/2324 | Loss: 5.9076\n",
      "Epoch: 062/100\n",
      "Train MAE: 7.00 | Validation MAE: 11.82\n",
      "Time elapsed: 11.28 min\n",
      "Epoch: 063/100 | Batch 400/2324 | Loss: 4.5896\n",
      "Epoch: 063/100 | Batch 800/2324 | Loss: 7.9455\n",
      "Epoch: 063/100 | Batch 1200/2324 | Loss: 6.2784\n",
      "Epoch: 063/100 | Batch 1600/2324 | Loss: 7.4142\n",
      "Epoch: 063/100 | Batch 2000/2324 | Loss: 7.5572\n",
      "Epoch: 063/100 | Batch 2324/2324 | Loss: 6.0497\n",
      "Epoch: 063/100\n",
      "Train MAE: 7.35 | Validation MAE: 12.32\n",
      "Time elapsed: 11.34 min\n",
      "Epoch: 064/100 | Batch 400/2324 | Loss: 8.2259\n",
      "Epoch: 064/100 | Batch 800/2324 | Loss: 8.6584\n",
      "Epoch: 064/100 | Batch 1200/2324 | Loss: 8.3559\n",
      "Epoch: 064/100 | Batch 1600/2324 | Loss: 6.6771\n",
      "Epoch: 064/100 | Batch 2000/2324 | Loss: 7.1264\n",
      "Epoch: 064/100 | Batch 2324/2324 | Loss: 6.0502\n",
      "Epoch: 064/100\n",
      "Train MAE: 7.04 | Validation MAE: 11.11\n",
      "Time elapsed: 11.37 min\n",
      "Epoch: 065/100 | Batch 400/2324 | Loss: 5.7408\n",
      "Epoch: 065/100 | Batch 800/2324 | Loss: 7.7609\n",
      "Epoch: 065/100 | Batch 1200/2324 | Loss: 7.9695\n",
      "Epoch: 065/100 | Batch 1600/2324 | Loss: 5.2074\n",
      "Epoch: 065/100 | Batch 2000/2324 | Loss: 14.6405\n",
      "Epoch: 065/100 | Batch 2324/2324 | Loss: 9.7328\n",
      "Epoch: 065/100\n",
      "Train MAE: 9.47 | Validation MAE: 13.04\n",
      "Time elapsed: 26.17 min\n",
      "Epoch: 066/100 | Batch 400/2324 | Loss: 9.9481\n",
      "Epoch: 066/100 | Batch 800/2324 | Loss: 10.2605\n",
      "Epoch: 066/100 | Batch 1200/2324 | Loss: 12.9329\n",
      "Epoch: 066/100 | Batch 1600/2324 | Loss: 6.5010\n",
      "Epoch: 066/100 | Batch 2000/2324 | Loss: 6.1128\n",
      "Epoch: 066/100 | Batch 2324/2324 | Loss: 6.5964\n",
      "Epoch: 066/100\n",
      "Train MAE: 7.21 | Validation MAE: 12.39\n",
      "Time elapsed: 11.89 min\n",
      "Epoch: 067/100 | Batch 400/2324 | Loss: 3.4090\n",
      "Epoch: 067/100 | Batch 800/2324 | Loss: 8.4207\n",
      "Epoch: 067/100 | Batch 1200/2324 | Loss: 7.3457\n",
      "Epoch: 067/100 | Batch 1600/2324 | Loss: 4.4764\n",
      "Epoch: 067/100 | Batch 2000/2324 | Loss: 4.5951\n",
      "Epoch: 067/100 | Batch 2324/2324 | Loss: 10.5815\n",
      "Epoch: 067/100\n",
      "Train MAE: 6.93 | Validation MAE: 11.34\n",
      "Time elapsed: 11.13 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 068/100 | Batch 400/2324 | Loss: 14.8769\n",
      "Epoch: 068/100 | Batch 800/2324 | Loss: 4.8805\n",
      "Epoch: 068/100 | Batch 1200/2324 | Loss: 9.9033\n",
      "Epoch: 068/100 | Batch 1600/2324 | Loss: 5.6765\n",
      "Epoch: 068/100 | Batch 2000/2324 | Loss: 13.6181\n",
      "Epoch: 068/100 | Batch 2324/2324 | Loss: 11.2580\n",
      "Epoch: 068/100\n",
      "Train MAE: 8.78 | Validation MAE: 13.30\n",
      "Time elapsed: 11.17 min\n",
      "Epoch: 069/100 | Batch 400/2324 | Loss: 5.2833\n",
      "Epoch: 069/100 | Batch 800/2324 | Loss: 7.1050\n",
      "Epoch: 069/100 | Batch 1200/2324 | Loss: 10.0690\n",
      "Epoch: 069/100 | Batch 1600/2324 | Loss: 7.1880\n",
      "Epoch: 069/100 | Batch 2000/2324 | Loss: 6.5123\n",
      "Epoch: 069/100 | Batch 2324/2324 | Loss: 8.3595\n",
      "Epoch: 069/100\n",
      "Train MAE: 7.80 | Validation MAE: 12.02\n",
      "Time elapsed: 11.14 min\n",
      "Epoch: 070/100 | Batch 400/2324 | Loss: 6.8935\n",
      "Epoch: 070/100 | Batch 800/2324 | Loss: 6.4129\n",
      "Epoch: 070/100 | Batch 1200/2324 | Loss: 5.7390\n",
      "Epoch: 070/100 | Batch 1600/2324 | Loss: 7.7507\n",
      "Epoch: 070/100 | Batch 2000/2324 | Loss: 4.2910\n",
      "Epoch: 070/100 | Batch 2324/2324 | Loss: 6.9440\n",
      "Epoch: 070/100\n",
      "Train MAE: 6.66 | Validation MAE: 12.29\n",
      "Time elapsed: 11.18 min\n",
      "Epoch: 071/100 | Batch 400/2324 | Loss: 7.2556\n",
      "Epoch: 071/100 | Batch 800/2324 | Loss: 17.9642\n",
      "Epoch: 071/100 | Batch 1200/2324 | Loss: 5.8630\n",
      "Epoch: 071/100 | Batch 1600/2324 | Loss: 6.4048\n",
      "Epoch: 071/100 | Batch 2000/2324 | Loss: 7.3191\n",
      "Epoch: 071/100 | Batch 2324/2324 | Loss: 7.2212\n",
      "Epoch: 071/100\n",
      "Train MAE: 8.33 | Validation MAE: 13.28\n",
      "Time elapsed: 11.21 min\n",
      "Epoch: 072/100 | Batch 400/2324 | Loss: 5.2214\n",
      "Epoch: 072/100 | Batch 800/2324 | Loss: 5.6753\n",
      "Epoch: 072/100 | Batch 1200/2324 | Loss: 6.2117\n",
      "Epoch: 072/100 | Batch 1600/2324 | Loss: 7.4344\n",
      "Epoch: 072/100 | Batch 2000/2324 | Loss: 6.2239\n",
      "Epoch: 072/100 | Batch 2324/2324 | Loss: 4.4132\n",
      "Epoch: 072/100\n",
      "Train MAE: 6.77 | Validation MAE: 12.48\n",
      "Time elapsed: 11.16 min\n",
      "Epoch: 073/100 | Batch 400/2324 | Loss: 5.2059\n",
      "Epoch: 073/100 | Batch 800/2324 | Loss: 10.4052\n",
      "Epoch: 073/100 | Batch 1200/2324 | Loss: 10.7810\n",
      "Epoch: 073/100 | Batch 1600/2324 | Loss: 5.9522\n",
      "Epoch: 073/100 | Batch 2000/2324 | Loss: 7.4534\n",
      "Epoch: 073/100 | Batch 2324/2324 | Loss: 5.5322\n",
      "Epoch: 073/100\n",
      "Train MAE: 8.67 | Validation MAE: 12.36\n",
      "Time elapsed: 11.15 min\n",
      "Epoch: 074/100 | Batch 400/2324 | Loss: 4.2357\n",
      "Epoch: 074/100 | Batch 800/2324 | Loss: 5.7623\n",
      "Epoch: 074/100 | Batch 1200/2324 | Loss: 5.6888\n",
      "Epoch: 074/100 | Batch 1600/2324 | Loss: 5.3335\n",
      "Epoch: 074/100 | Batch 2000/2324 | Loss: 11.8600\n",
      "Epoch: 074/100 | Batch 2324/2324 | Loss: 8.1789\n",
      "Epoch: 074/100\n",
      "Train MAE: 7.10 | Validation MAE: 11.31\n",
      "Time elapsed: 11.19 min\n",
      "Epoch: 075/100 | Batch 400/2324 | Loss: 3.7640\n",
      "Epoch: 075/100 | Batch 800/2324 | Loss: 8.5090\n",
      "Epoch: 075/100 | Batch 1200/2324 | Loss: 4.5447\n",
      "Epoch: 075/100 | Batch 1600/2324 | Loss: 5.7710\n",
      "Epoch: 075/100 | Batch 2000/2324 | Loss: 5.9653\n",
      "Epoch: 075/100 | Batch 2324/2324 | Loss: 7.8754\n",
      "Epoch: 075/100\n",
      "Train MAE: 7.98 | Validation MAE: 12.89\n",
      "Time elapsed: 11.22 min\n",
      "Epoch: 076/100 | Batch 400/2324 | Loss: 7.7419\n",
      "Epoch: 076/100 | Batch 800/2324 | Loss: 5.2598\n",
      "Epoch: 076/100 | Batch 1200/2324 | Loss: 8.7283\n",
      "Epoch: 076/100 | Batch 1600/2324 | Loss: 3.5999\n",
      "Epoch: 076/100 | Batch 2000/2324 | Loss: 6.0750\n",
      "Epoch: 076/100 | Batch 2324/2324 | Loss: 8.6313\n",
      "Epoch: 076/100\n",
      "Train MAE: 6.76 | Validation MAE: 11.65\n",
      "Time elapsed: 11.20 min\n",
      "Epoch: 077/100 | Batch 400/2324 | Loss: 7.4177\n",
      "Epoch: 077/100 | Batch 800/2324 | Loss: 5.9984\n",
      "Epoch: 077/100 | Batch 1200/2324 | Loss: 5.9841\n",
      "Epoch: 077/100 | Batch 1600/2324 | Loss: 6.2181\n",
      "Epoch: 077/100 | Batch 2000/2324 | Loss: 6.5996\n",
      "Epoch: 077/100 | Batch 2324/2324 | Loss: 7.3603\n",
      "Epoch: 077/100\n",
      "Train MAE: 6.02 | Validation MAE: 11.68\n",
      "Time elapsed: 11.20 min\n",
      "Epoch: 078/100 | Batch 400/2324 | Loss: 8.0975\n",
      "Epoch: 078/100 | Batch 800/2324 | Loss: 5.3784\n",
      "Epoch: 078/100 | Batch 1200/2324 | Loss: 7.0303\n",
      "Epoch: 078/100 | Batch 1600/2324 | Loss: 6.3217\n",
      "Epoch: 078/100 | Batch 2000/2324 | Loss: 8.5747\n",
      "Epoch: 078/100 | Batch 2324/2324 | Loss: 5.2677\n",
      "Epoch: 078/100\n",
      "Train MAE: 6.09 | Validation MAE: 12.08\n",
      "Time elapsed: 11.19 min\n",
      "Epoch: 079/100 | Batch 400/2324 | Loss: 5.6434\n",
      "Epoch: 079/100 | Batch 800/2324 | Loss: 6.9755\n",
      "Epoch: 079/100 | Batch 1200/2324 | Loss: 7.6866\n",
      "Epoch: 079/100 | Batch 1600/2324 | Loss: 7.4776\n",
      "Epoch: 079/100 | Batch 2000/2324 | Loss: 11.8124\n",
      "Epoch: 079/100 | Batch 2324/2324 | Loss: 5.0523\n",
      "Epoch: 079/100\n",
      "Train MAE: 7.13 | Validation MAE: 12.24\n",
      "Time elapsed: 11.18 min\n",
      "Epoch: 080/100 | Batch 400/2324 | Loss: 6.4797\n",
      "Epoch: 080/100 | Batch 800/2324 | Loss: 13.2706\n",
      "Epoch: 080/100 | Batch 1200/2324 | Loss: 5.6583\n",
      "Epoch: 080/100 | Batch 1600/2324 | Loss: 5.8292\n",
      "Epoch: 080/100 | Batch 2000/2324 | Loss: 5.5171\n",
      "Epoch: 080/100 | Batch 2324/2324 | Loss: 10.2786\n",
      "Epoch: 080/100\n",
      "Train MAE: 7.01 | Validation MAE: 12.10\n",
      "Time elapsed: 11.20 min\n",
      "Epoch: 081/100 | Batch 400/2324 | Loss: 6.9883\n",
      "Epoch: 081/100 | Batch 800/2324 | Loss: 6.4679\n",
      "Epoch: 081/100 | Batch 1200/2324 | Loss: 5.8113\n",
      "Epoch: 081/100 | Batch 1600/2324 | Loss: 8.0278\n",
      "Epoch: 081/100 | Batch 2000/2324 | Loss: 7.3526\n",
      "Epoch: 081/100 | Batch 2324/2324 | Loss: 7.0896\n",
      "Epoch: 081/100\n",
      "Train MAE: 7.30 | Validation MAE: 12.56\n",
      "Time elapsed: 11.19 min\n",
      "Epoch: 082/100 | Batch 400/2324 | Loss: 6.0621\n",
      "Epoch: 082/100 | Batch 800/2324 | Loss: 5.6056\n",
      "Epoch: 082/100 | Batch 1200/2324 | Loss: 5.7641\n",
      "Epoch: 082/100 | Batch 1600/2324 | Loss: 4.1765\n",
      "Epoch: 082/100 | Batch 2000/2324 | Loss: 4.7541\n",
      "Epoch: 082/100 | Batch 2324/2324 | Loss: 7.8817\n",
      "Epoch: 082/100\n",
      "Train MAE: 7.12 | Validation MAE: 12.27\n",
      "Time elapsed: 11.21 min\n",
      "Epoch: 083/100 | Batch 400/2324 | Loss: 9.2246\n",
      "Epoch: 083/100 | Batch 800/2324 | Loss: 6.6856\n",
      "Epoch: 083/100 | Batch 1200/2324 | Loss: 9.3049\n",
      "Epoch: 083/100 | Batch 1600/2324 | Loss: 2.7348\n",
      "Epoch: 083/100 | Batch 2000/2324 | Loss: 5.3070\n",
      "Epoch: 083/100 | Batch 2324/2324 | Loss: 5.4818\n",
      "Epoch: 083/100\n",
      "Train MAE: 6.69 | Validation MAE: 12.41\n",
      "Time elapsed: 11.21 min\n",
      "Epoch: 084/100 | Batch 400/2324 | Loss: 8.4511\n",
      "Epoch: 084/100 | Batch 800/2324 | Loss: 9.8027\n",
      "Epoch: 084/100 | Batch 1200/2324 | Loss: 10.9489\n",
      "Epoch: 084/100 | Batch 1600/2324 | Loss: 4.8427\n",
      "Epoch: 084/100 | Batch 2000/2324 | Loss: 13.1354\n",
      "Epoch: 084/100 | Batch 2324/2324 | Loss: 6.3810\n",
      "Epoch: 084/100\n",
      "Train MAE: 6.90 | Validation MAE: 11.67\n",
      "Time elapsed: 11.20 min\n",
      "Epoch: 085/100 | Batch 400/2324 | Loss: 6.3486\n",
      "Epoch: 085/100 | Batch 800/2324 | Loss: 6.8559\n",
      "Epoch: 085/100 | Batch 1200/2324 | Loss: 7.4566\n",
      "Epoch: 085/100 | Batch 1600/2324 | Loss: 6.3445\n",
      "Epoch: 085/100 | Batch 2000/2324 | Loss: 6.6625\n",
      "Epoch: 085/100 | Batch 2324/2324 | Loss: 3.0821\n",
      "Epoch: 085/100\n",
      "Train MAE: 5.72 | Validation MAE: 12.65\n",
      "Time elapsed: 11.24 min\n",
      "Epoch: 086/100 | Batch 400/2324 | Loss: 8.0439\n",
      "Epoch: 086/100 | Batch 800/2324 | Loss: 6.9533\n",
      "Epoch: 086/100 | Batch 1200/2324 | Loss: 7.9078\n",
      "Epoch: 086/100 | Batch 1600/2324 | Loss: 5.5592\n",
      "Epoch: 086/100 | Batch 2000/2324 | Loss: 12.6622\n",
      "Epoch: 086/100 | Batch 2324/2324 | Loss: 4.0915\n",
      "Epoch: 086/100\n",
      "Train MAE: 6.67 | Validation MAE: 11.61\n",
      "Time elapsed: 11.28 min\n",
      "Epoch: 087/100 | Batch 400/2324 | Loss: 6.2795\n",
      "Epoch: 087/100 | Batch 800/2324 | Loss: 4.0843\n",
      "Epoch: 087/100 | Batch 1200/2324 | Loss: 5.1262\n",
      "Epoch: 087/100 | Batch 1600/2324 | Loss: 7.2381\n",
      "Epoch: 087/100 | Batch 2000/2324 | Loss: 4.8279\n",
      "Epoch: 087/100 | Batch 2324/2324 | Loss: 8.9542\n",
      "Epoch: 087/100\n",
      "Train MAE: 7.10 | Validation MAE: 12.52\n",
      "Time elapsed: 11.18 min\n",
      "Epoch: 088/100 | Batch 400/2324 | Loss: 4.2893\n",
      "Epoch: 088/100 | Batch 800/2324 | Loss: 6.7721\n",
      "Epoch: 088/100 | Batch 1200/2324 | Loss: 5.0784\n",
      "Epoch: 088/100 | Batch 1600/2324 | Loss: 7.6648\n",
      "Epoch: 088/100 | Batch 2000/2324 | Loss: 4.3956\n",
      "Epoch: 088/100 | Batch 2324/2324 | Loss: 6.3255\n",
      "Epoch: 088/100\n",
      "Train MAE: 7.48 | Validation MAE: 12.89\n",
      "Time elapsed: 11.21 min\n",
      "Epoch: 089/100 | Batch 400/2324 | Loss: 5.2295\n",
      "Epoch: 089/100 | Batch 800/2324 | Loss: 6.8160\n",
      "Epoch: 089/100 | Batch 1200/2324 | Loss: 4.2222\n",
      "Epoch: 089/100 | Batch 1600/2324 | Loss: 6.5713\n",
      "Epoch: 089/100 | Batch 2000/2324 | Loss: 6.6954\n",
      "Epoch: 089/100 | Batch 2324/2324 | Loss: 5.7364\n",
      "Epoch: 089/100\n",
      "Train MAE: 7.27 | Validation MAE: 12.51\n",
      "Time elapsed: 11.21 min\n",
      "Epoch: 090/100 | Batch 400/2324 | Loss: 4.6401\n",
      "Epoch: 090/100 | Batch 800/2324 | Loss: 5.6301\n",
      "Epoch: 090/100 | Batch 1200/2324 | Loss: 9.8248\n",
      "Epoch: 090/100 | Batch 1600/2324 | Loss: 6.4408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 090/100 | Batch 2000/2324 | Loss: 6.2745\n",
      "Epoch: 090/100 | Batch 2324/2324 | Loss: 6.4223\n",
      "Epoch: 090/100\n",
      "Train MAE: 5.71 | Validation MAE: 12.01\n",
      "Time elapsed: 11.21 min\n",
      "Epoch: 091/100 | Batch 400/2324 | Loss: 6.4996\n",
      "Epoch: 091/100 | Batch 800/2324 | Loss: 6.8287\n",
      "Epoch: 091/100 | Batch 1200/2324 | Loss: 5.5672\n",
      "Epoch: 091/100 | Batch 1600/2324 | Loss: 7.2925\n",
      "Epoch: 091/100 | Batch 2000/2324 | Loss: 5.9996\n",
      "Epoch: 091/100 | Batch 2324/2324 | Loss: 11.7878\n",
      "Epoch: 091/100\n",
      "Train MAE: 5.89 | Validation MAE: 10.97\n",
      "Time elapsed: 11.26 min\n",
      "Epoch: 092/100 | Batch 400/2324 | Loss: 9.2565\n",
      "Epoch: 092/100 | Batch 800/2324 | Loss: 4.9558\n",
      "Epoch: 092/100 | Batch 1200/2324 | Loss: 5.5935\n",
      "Epoch: 092/100 | Batch 1600/2324 | Loss: 3.7780\n",
      "Epoch: 092/100 | Batch 2000/2324 | Loss: 5.3744\n",
      "Epoch: 092/100 | Batch 2324/2324 | Loss: 8.5201\n",
      "Epoch: 092/100\n",
      "Train MAE: 6.48 | Validation MAE: 11.88\n",
      "Time elapsed: 11.27 min\n",
      "Epoch: 093/100 | Batch 400/2324 | Loss: 3.2080\n",
      "Epoch: 093/100 | Batch 800/2324 | Loss: 6.1006\n",
      "Epoch: 093/100 | Batch 1200/2324 | Loss: 3.7992\n",
      "Epoch: 093/100 | Batch 1600/2324 | Loss: 7.0774\n",
      "Epoch: 093/100 | Batch 2000/2324 | Loss: 12.8172\n",
      "Epoch: 093/100 | Batch 2324/2324 | Loss: 7.9187\n",
      "Epoch: 093/100\n",
      "Train MAE: 6.25 | Validation MAE: 12.23\n",
      "Time elapsed: 11.22 min\n",
      "Epoch: 094/100 | Batch 400/2324 | Loss: 6.5830\n",
      "Epoch: 094/100 | Batch 800/2324 | Loss: 8.8808\n",
      "Epoch: 094/100 | Batch 1200/2324 | Loss: 5.9973\n",
      "Epoch: 094/100 | Batch 1600/2324 | Loss: 5.8304\n",
      "Epoch: 094/100 | Batch 2000/2324 | Loss: 8.0856\n",
      "Epoch: 094/100 | Batch 2324/2324 | Loss: 4.5085\n",
      "Epoch: 094/100\n",
      "Train MAE: 6.02 | Validation MAE: 11.97\n",
      "Time elapsed: 11.27 min\n",
      "Epoch: 095/100 | Batch 400/2324 | Loss: 5.5314\n",
      "Epoch: 095/100 | Batch 800/2324 | Loss: 6.9803\n",
      "Epoch: 095/100 | Batch 1200/2324 | Loss: 7.4353\n",
      "Epoch: 095/100 | Batch 1600/2324 | Loss: 9.4987\n",
      "Epoch: 095/100 | Batch 2000/2324 | Loss: 7.7300\n",
      "Epoch: 095/100 | Batch 2324/2324 | Loss: 5.6961\n",
      "Epoch: 095/100\n",
      "Train MAE: 6.80 | Validation MAE: 12.60\n",
      "Time elapsed: 12.13 min\n",
      "Epoch: 096/100 | Batch 400/2324 | Loss: 4.4024\n",
      "Epoch: 096/100 | Batch 800/2324 | Loss: 5.9519\n",
      "Epoch: 096/100 | Batch 1200/2324 | Loss: 8.2034\n",
      "Epoch: 096/100 | Batch 1600/2324 | Loss: 6.8243\n",
      "Epoch: 096/100 | Batch 2000/2324 | Loss: 3.3968\n",
      "Epoch: 096/100 | Batch 2324/2324 | Loss: 7.5370\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "l = []\n",
    "train_mae_list, valid_mae_list = [], []\n",
    "train_mse_list, valid_mse_list = [], []\n",
    "train_acc_list, valid_acc_list = [], []\n",
    "\n",
    "for epoch in range(int(epochs)):\n",
    "    start = time.time()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, inputDim))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        l.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (not (i+1) % 400) or (i+1 == len(train_loader)):\n",
    "            print (f'Epoch: {epoch+1:03d}/{epochs:03d} | '\n",
    "                   f'Batch {i+1:03d}/{len(train_loader):03d} |' \n",
    "                   f' Loss: {loss.item():.4f}')\n",
    "    with torch.set_grad_enabled(False):\n",
    "        \n",
    "        train_mae, train_mse, train_acc = compute_mae_mse_acc(model, train_loader)\n",
    "        valid_mae, valid_mse, valid_acc = compute_mae_mse_acc(model, valid_loader)\n",
    " \n",
    "        print(f'Epoch: {epoch+1:03d}/{epochs:03d}\\n'\n",
    "              f'Train MAE: {train_mae:.2f} | Validation MAE: {valid_mae:.2f}')\n",
    "        \n",
    "        train_mae_list.append(train_mae)\n",
    "        valid_mae_list.append(valid_mae)\n",
    "        train_mse_list.append(train_mse)\n",
    "        valid_mse_list.append(valid_mse)\n",
    "        train_acc_list.append(train_acc)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        \n",
    "    elapsed = (time.time() - start)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "\n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l, label='Minibatch cost')\n",
    "plt.plot(np.convolve(l, np.ones(200,)/200, mode='valid'), label='Running average')\n",
    "\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, epochs+1), train_mae_list, label='Training')\n",
    "plt.plot(np.arange(1, epochs+1), valid_mae_list, label='Validation')\n",
    "plt.ylim(2, 15)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, NUM_EPOCHS+1), train_acc_list, label='Training')\n",
    "plt.plot(np.arange(1, NUM_EPOCHS+1), valid_acc_list, label='Validation')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "    test_mae, test_mse, test_acc = compute_mae_mse_acc(model=model, data_loader=test_loader)\n",
    "    \n",
    "print(f'Test MAE: {test_mae:.2f}')\n",
    "print(f'Test MSE: {test_mse:.2f}')\n",
    "print(f'Test ACC: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'param2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 250*250))\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "#     total+= labels.size(0)\n",
    "    print(predicted)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = linearRegression(inputDim, outputDim)\n",
    "model2.load_state_dict(torch.load('param'))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 250*250))\n",
    "    outputs = model2(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "#     total+= labels.size(0)\n",
    "    print(predicted)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "    test_mae, test_mse, test_acc = compute_mae_mse_acc(model=model2, data_loader=test_loader)\n",
    "    \n",
    "print(f'Test MAE: {test_mae:.2f}')\n",
    "print(f'Test MSE: {test_mse:.2f}')\n",
    "print(f'Test ACC: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
